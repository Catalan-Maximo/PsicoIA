## ğŸ¯ Objetivo del Proyecto

Desarrollar un chatbot de apoyo emocional accesible 24/7 que:
- Escuche activamente sin juzgar
- Valide emocionalmente al usuario
- Ofrezca estrategias de autocuidado
- Derive a profesionales cuando sea necesario

**âš ï¸ Importante**: No sustituye atenciÃ³n profesional de salud mental.

---

## ğŸ—ï¸ Arquitectura TÃ©cnica

### Componentes principales

1. **Servidor TCP (`app/`)**
   - Puerto: 5001
   - Maneja lÃ³gica de negocio
   - Conexiones persistentes con usuarios
   - IntegraciÃ³n con LLM

2. **Gateway WebSocket (`gateway/`)**
   - Puerto: 8765
   - Puente entre navegador y servidor TCP
   - Mapeo 1:1 de sesiones

3. **Cliente Web (`web/`)**
   - HTML/CSS/JavaScript puro
   - Interfaz moderna con tema claro/oscuro
   - Sin dependencias externas

### Flujo de datos

```
Usuario â†’ WebSocket â†’ Gateway â†’ TCP â†’ Servidor â†’ LLM API
                                         â†“
Usuario â† WebSocket â† Gateway â† TCP â† Respuesta
```

---

## âš™ï¸ CaracterÃ­sticas TÃ©cnicas Destacadas

### Concurrencia y escalabilidad
- **asyncio**: I/O no bloqueante, miles de usuarios simultÃ¡neos
- **Coroutines**: Una por cada conexiÃ³n (sin hilos)
- **SemÃ¡foro global**: Limita requests simultÃ¡neos al LLM
- **Rate limiting**: Ventana deslizante por usuario
---

## ğŸ”§ Stack TecnolÃ³gico

|     Componente    |        TecnologÃ­a        |          PropÃ³sito         |
|-------------------|--------------------------|----------------------------|
|   **Lenguaje**    |        Python 3.12       |       Backend completo     |
|   **Async I/O**   |          asyncio         |  Concurrencia sin threads  |
|  **HTTP Client**  |           httpx          |   Llamadas async al LLM    |
|   **WebSocket**   |        websockets        | Gateway navegadorâ†”servidor |
| **ConfiguraciÃ³n** |     pydantic-settings    |     ValidaciÃ³n de config   |
|    **Logging**    |     logging stdlib       |        Trazabilidad        |
|  **Contenedores** |     Docker + Compose     |          Deployment        |
|    **Frontend**   |      HTML5/CSS3/JS       |          Cliente web       |
|       **LLM**     | Groq (OpenAI compatible) |       IA conversacional    |

---

## ğŸ“ Estructura del CÃ³digo

```
PsicoIA/
â”œâ”€â”€ app/                       # Servidor TCP
â”‚   â”œâ”€â”€ server.py              # Punto de entrada
â”‚   â”œâ”€â”€ client_handler.py      # LÃ³gica por conexiÃ³n
â”‚   â”œâ”€â”€ config.py              # ConfiguraciÃ³n centralizada
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â””â”€â”€ promptgeneral.py   # Prompt del sistema
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ llm_client.py      # Cliente LLM async
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py          # Logging
â”‚       â””â”€â”€ rate_limiter.py    # Control de tasa
â”œâ”€â”€ gateway/
â”‚   â””â”€â”€ ws_gateway.py          # Gateway WebSocketâ†”TCP
â”œâ”€â”€ web/
â”‚   â””â”€â”€ client_web.html        # Cliente navegador
â”œâ”€â”€ examples/                  # Scripts de ejemplo
â”‚   â”œâ”€â”€ tcp_client.py          # Cliente simple
â”‚   â””â”€â”€ async_tcp_client.py    # Pruebas de carga
â”œâ”€â”€ docker-compose.yml         # OrquestaciÃ³n
â”œâ”€â”€ Dockerfile                 # Imagen base
â”œâ”€â”€ requirements.txt           # Dependencias
â””â”€â”€ .env.example               # Template config
```












---

## ğŸ“‚ ExplicaciÃ³n detallada de cada archivo

### **RaÃ­z del proyecto**

#### `docker-compose.yml`
- **QuÃ© hace**: Orquesta los dos servicios principales (`app` y `gateway`) en contenedores Docker.
- **FunciÃ³n**: Define la configuraciÃ³n de red, puertos, variables de entorno y dependencias entre servicios.
- **Servicios**:
  - `app`: Servidor TCP (puerto 5001) que maneja la lÃ³gica del chatbot y conexiones al LLM.
  - `gateway`: Gateway WebSocket (puerto 8765) que actÃºa como puente entre navegador y servidor TCP.

#### `Dockerfile`
- **QuÃ© hace**: Define la imagen Docker base para ambos servicios.
- **FunciÃ³n**: Instala Python 3.12, copia dependencias (`requirements.txt`), instala paquetes y expone los puertos 5001 y 8765.

#### `.env.example`
- **QuÃ© hace**: Plantilla con todas las variables de entorno necesarias.
- **Variables clave**:
  - `GROQ_API_KEY`: Clave API para el proveedor LLM.
  - `MODEL_NAME`: Modelo a usar (ej: `meta-llama/llama-3.1-8b-instant`).
  - `APP_PORT`, `MAX_IN_FLIGHT`, `RATE_MAX_MESSAGES`: ConfiguraciÃ³n del servidor.
  - `LLM_TEMPERATURE`, `LLM_MAX_TOKENS`: ParÃ¡metros del modelo.

---

### **Directorio `app/`**

#### `app/server.py`
- **QuÃ© hace**: Punto de entrada del servidor TCP asÃ­ncrono.
- **FunciÃ³n**: 
  - Inicia un servidor TCP usando `asyncio.start_server()`.
  - Escucha conexiones en `APP_HOST:APP_PORT` (por defecto `0.0.0.0:5001`).
  - Por cada conexiÃ³n entrante, delega el manejo a `handle_client()`.
- **Modelo de concurrencia**: 
  - Un solo proceso/hilo con asyncio (I/O no bloqueante).
  - Cada cliente obtiene su propia coroutine independiente.

#### `app/client_handler.py`
- **QuÃ© hace**: Maneja la conexiÃ³n TCP de cada cliente individual.
- **FunciÃ³n**:
  - Asigna un ID Ãºnico por conexiÃ³n (`Usuario-1`, `Usuario-2`, etc.).
  - Lee mensajes del cliente lÃ­nea por lÃ­nea de forma asÃ­ncrona.
  - Aplica **rate limiting** por usuario (ventana deslizante).
  - Usa un **semÃ¡foro global** (`MAX_IN_FLIGHT`) para limitar llamadas simultÃ¡neas al LLM.
  - Genera `trace_id` para trazabilidad de cada mensaje (`Usuario-X:mY`).
  - Llama a `llm_generate()` de forma asÃ­ncrona y devuelve la respuesta al cliente.
- **Aislamiento**: Cada conexiÃ³n es independiente; no se mezclan estados entre usuarios.

#### `app/config.py`
- **QuÃ© hace**: Centraliza la configuraciÃ³n del proyecto usando Pydantic Settings.
- **FunciÃ³n**:
  - Lee variables desde `.env` automÃ¡ticamente.
  - Valida tipos y proporciona valores por defecto.
  - Exporta objeto `settings` usado en toda la aplicaciÃ³n.

---

### **Directorio `app/services/`**

#### `app/services/llm_client.py`
- **QuÃ© hace**: Cliente asÃ­ncrono para interactuar con el LLM (Groq/OpenAI compatible).
- **Funciones principales**:
  - `llm_generate(user_text, trace_id, conversation_id)`: EnvÃ­a mensaje al LLM y recibe respuesta.
  - Gestiona **historial de conversaciÃ³n en RAM** por usuario (`conversation_id`).
  - Implementa **ventana deslizante de tokens** para no exceder lÃ­mite de contexto.
  - Aplica **reintentos con backoff exponencial** en caso de errores 429/5xx.
  - Usa `httpx.AsyncClient` para llamadas HTTP no bloqueantes.
- **Trazabilidad**: Incluye `trace_id` en headers y logs para correlacionar requests.
- **Fallback**: Si no hay API key, retorna mensaje empÃ¡tico offline.

---

### **Directorio `app/prompts/`**

#### `app/prompts/promptgeneral.py`
- **QuÃ© hace**: Define el prompt del sistema para el LLM.
- **FunciÃ³n**:
  - Constante `SYSTEM_PROMPT` con instrucciones detalladas para el asistente psicolÃ³gico.
  - Diccionario de palabras clave (alarma, ansiedad, depresiÃ³n, motivaciÃ³n).
  - GuÃ­as de tono, estructura de respuesta y restricciones Ã©ticas.
- **Importancia**: Este archivo define la "personalidad" y comportamiento del chatbot.

---

### **Directorio `app/utils/`**

#### `app/utils/logger.py`
- **QuÃ© hace**: Configura el sistema de logging del proyecto.
- **FunciÃ³n**:
  - Usa `logging.basicConfig()` con formato estÃ¡ndar (timestamp, nivel, mensaje).
  - Exporta `get_logger()` para obtener loggers por mÃ³dulo.

#### `app/utils/rate_limiter.py`
- **QuÃ© hace**: Implementa rate limiting por ventana deslizante.
- **Clase `SlidingWindowLimiter`**:
  - ParÃ¡metros: `max_events` (mÃ¡ximo mensajes), `window_seconds` (ventana temporal).
  - MÃ©todo `allow()`: Retorna `True` si el usuario puede enviar mensaje, `False` si excede lÃ­mite.
  - Almacena timestamps en `deque` y limpia eventos antiguos automÃ¡ticamente.
- **Uso**: Previene flooding de un solo usuario sin afectar a otros.

---

### **Directorio `gateway/`**

#### `gateway/ws_gateway.py`
- **QuÃ© hace**: Gateway WebSocket que conecta navegadores con el servidor TCP.
- **FunciÃ³n**:
  - Escucha conexiones WebSocket en `WS_HOST:WS_PORT` (por defecto `0.0.0.0:8765`).
  - Por **cada cliente WebSocket**, abre **una conexiÃ³n TCP dedicada** al servidor `app`.
  - Mapeo **1:1** (WS â†” TCP) garantiza aislamiento de sesiones.
  - Implementa dos coroutines asÃ­ncronas:
    - `ws_reader()`: Lee del WebSocket y escribe al TCP.
    - `tcp_reader()`: Lee del TCP y escribe al WebSocket.
- **ConfiguraciÃ³n Docker**: En compose, `TCP_HOST='app'` usa la red interna Docker.
- **ConfiguraciÃ³n local**: `TCP_HOST='127.0.0.1'` para testing sin Docker.

---







## ğŸ’¡ Conceptos Clave para la PresentaciÃ³n

### 1. **Modelo de Concurrencia**
- "Un solo proceso Python maneja cientos de usuarios simultÃ¡neos"
- "asyncio permite I/O no bloqueante sin threads ni procesos"
- "Cada conexiÃ³n es una coroutine independiente"

### 2. **Arquitectura Desacoplada**
- "Servidor TCP puro (sin web dependencies)"
- "Gateway como capa de transporte intercambiable"

### 3. **Escalabilidad Horizontal**
- "MÃºltiples instancias del servidor con load balancer"
- "Redis para compartir historiales entre instancias"
- "Stateless design facilita scaling"

### 4. **GestiÃ³n de Contexto LLM**
- "Ventana deslizante de tokens"
- "Solo mensajes recientes al LLM"
- "Historial completo en RAM por si se necesita"

---

## ğŸ¤ Puntos Clave para Defender

### Decisiones de diseÃ±o

**Â¿Por quÃ© asyncio en lugar de threads?**
- MÃ¡s liviano: miles de coroutines vs cientos de threads
- Mejor control: event loop explÃ­cito
- Ideal para I/O-bound tasks (espera de red)

**Â¿Por quÃ© separar TCP y WebSocket?**
- SeparaciÃ³n de responsabilidades
- Testing mÃ¡s simple (TCP es mÃ¡s directo)
- Flexibilidad: agregar otros protocolos sin tocar lÃ³gica

**Â¿Por quÃ© historial en RAM y no DB?**
- Prototipo funcional mÃ¡s rÃ¡pido
- Suficiente para demo y testing
- FÃ¡cil migrar a DB despuÃ©s (interfaz ya definida)

**Â¿Por quÃ© Docker?**
- Reproducibilidad del entorno
- Facilita deployment
- AÃ­sla dependencias del sistema host

---