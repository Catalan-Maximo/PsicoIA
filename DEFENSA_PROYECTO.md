## Descripci√≥n General del Proyecto

**PsicoIA** es un sistema de chatbot de apoyo emocional basado en arquitectura cliente-servidor que utiliza **TCP** como protocolo de comunicaci√≥n. El sistema est√° dise√±ado para ser **multiusuario**, **concurrente** y **escalable**, permitiendo que m√∫ltiples usuarios se conecten simult√°neamente y reciban acompa√±amiento emocional personalizado.

---

## ‚úÖ Operaciones Admitidas

El sistema implementa las siguientes caracter√≠sticas funcionales:

1. ‚úÖ **Conversaciones simult√°neas** entre m√∫ltiples usuarios con estado de sesi√≥n aislado
2. ‚úÖ **Limitaci√≥n de velocidad por usuario** (ventana deslizante y n√∫mero de mensajes configurables)
3. ‚úÖ **Control global de contrapresi√≥n** (n√∫mero m√°ximo de solicitudes LLM simult√°neas configurables mediante sem√°foro)
4. ‚úÖ **Reintento autom√°tico con backoff exponencial** para fallos transitorios de la API
5. ‚úÖ **Ajuste de ventana de tokens** para adaptar el historial de conversaci√≥n a los l√≠mites del contexto del modelo
6. ‚úÖ **Degradaci√≥n controlada** cuando la API de LLM no est√° disponible (respuesta de fallback)
7. ‚úÖ **Opciones de conectividad duales**: WebSocket (navegadores) y TCP directo (clientes nativos)
8. ‚úÖ **Generaci√≥n de trace_id** para depuraci√≥n de solicitudes en todos los componentes
9. ‚úÖ **Observabilidad completa** con logs estructurados y m√©tricas de latencia
10. ‚úÖ **Containerizaci√≥n** con Docker para reproducibilidad y despliegue simple

---

## ‚ö†Ô∏è Limitaciones Conocidas

El sistema tiene las siguientes limitaciones por dise√±o (prototipo educativo):

1. ‚ö†Ô∏è **Historial en RAM**: Las conversaciones se pierden al reiniciar el servidor
2. ‚ö†Ô∏è **Sin autenticaci√≥n**: No hay sistema de cuentas de usuario ni control de acceso
3. ‚ö†Ô∏è **Sin persistencia**: No se guardan conversaciones en base de datos
4. ‚ö†Ô∏è **Instancia √∫nica**: Despliegue monol√≠tico sin escalado horizontal (requerir√≠a estado externo compartido)
5. ‚ö†Ô∏è **Sin cifrado TLS/SSL**: Las comunicaciones no est√°n cifradas (HTTP/WS plano)
6. ‚ö†Ô∏è **Identificaci√≥n temporal**: `Usuario-N` es secuencial y no persistente entre sesiones
7. ‚ö†Ô∏è **Sin recuperaci√≥n de sesi√≥n**: Si un cliente se desconecta, pierde su historial

**Nota**: Estas limitaciones son intencionales para mantener la simplicidad del prototipo educativo. Todas son resolubles con las extensiones mencionadas en la pregunta 12 ("¬øC√≥mo escalar√≠a el sistema para producci√≥n?").

---

### Objetivos del Proyecto

1. **Proporcionar apoyo emocional inmediato** mediante un chatbot inteligente
2. **Demostrar arquitectura de red robusta** con TCP y WebSocket
3. **Implementar concurrencia real** usando programaci√≥n as√≠ncrona
4. **Garantizar observabilidad** con sistema de logs y trazabilidad
5. **Facilitar el despliegue** mediante contenedores Docker

### Caracter√≠sticas Principales

- ‚úÖ **Servidor TCP Multiusuario**: Soporta m√∫ltiples conexiones simult√°neas
- ‚úÖ **Gateway WebSocket**: Permite que navegadores web se conecten al servidor TCP
- ‚úÖ **Concurrencia As√≠ncrona**: Uso de `asyncio` para manejar miles de conexiones
- ‚úÖ **Integraci√≥n con LLM**: Utiliza modelos de lenguaje (Groq/LLaMA) para respuestas inteligentes
- ‚úÖ **Rate Limiting**: Protecci√≥n contra flooding por usuario
- ‚úÖ **Historial Conversacional**: Mantiene contexto de la conversaci√≥n
- ‚úÖ **Containerizaci√≥n**: Deploy simple con Docker Compose
- ‚úÖ **Cliente Web Moderno**: Interface HTML/JS con dise√±o emp√°tico

---

## Arquitectura del Sistema

### Diagrama de Componentes

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Navegador     ‚îÇ
‚îÇ  (Cliente Web)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ WebSocket (ws://localhost:8765)
         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Gateway WS     ‚îÇ
‚îÇ   (Puerto 8765) ‚îÇ ‚óÑ‚îÄ‚îÄ Traduce WebSocket ‚Üî TCP
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ TCP (localhost:5001)
         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Servidor TCP   ‚îÇ
‚îÇ   (Puerto 5001) ‚îÇ ‚óÑ‚îÄ‚îÄ Maneja m√∫ltiples clientes
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚ñ∫ cliente_handler (coroutine por cliente)
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚ñ∫ llm_client (llamadas a API externa)
         ‚îÇ
         ‚îî‚îÄ‚îÄ‚ñ∫ rate_limiter (control de flooding)
```

### Flujo de Datos

1. **Usuario en navegador** ‚Üí escribe mensaje
2. **Cliente Web HTML/JS** ‚Üí env√≠a por WebSocket al gateway
3. **Gateway WS** ‚Üí traduce y reenv√≠a por TCP al servidor
4. **Servidor TCP** ‚Üí recibe, procesa y asigna al cliente_handler
5. **Client Handler** ‚Üí valida rate limit, mantiene historial
6. **LLM Client** ‚Üí env√≠a prompt a modelo de IA (Groq)
7. **Respuesta del LLM** ‚Üí vuelve por el camino inverso
8. **Usuario ve respuesta** en su navegador

---

## Estructura del Proyecto

```
PsicoIA/
‚îÇ
‚îú‚îÄ‚îÄ app/                          # N√∫cleo del servidor TCP
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ server.py                 # Punto de entrada del servidor TCP
‚îÇ   ‚îú‚îÄ‚îÄ client_handler.py         # Maneja cada cliente individual
‚îÇ   ‚îú‚îÄ‚îÄ config.py                 # Configuraci√≥n (variables de entorno)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_client.py         # Cliente para LLM (Groq/LLaMA)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.py             # Sistema de logging
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rate_limiter.py       # Control de tasa por usuario
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ prompts/
‚îÇ       ‚îú‚îÄ‚îÄ promptgeneral.py      # Prompt del sistema para el LLM
‚îÇ
‚îú‚îÄ‚îÄ gateway/                      # Gateway WebSocket ‚Üî TCP
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ ws_gateway.py             # Servidor WebSocket
‚îÇ
‚îú‚îÄ‚îÄ web/
‚îÇ   ‚îî‚îÄ‚îÄ client_web.html           # Cliente web (HTML + JS + CSS)
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml            # Orquestaci√≥n de contenedores
‚îú‚îÄ‚îÄ Dockerfile                    # Imagen del contenedor
‚îú‚îÄ‚îÄ requirements.txt              # Dependencias Python
‚îú‚îÄ‚îÄ .env                          # Variables de entorno (no en repo)
‚îî‚îÄ‚îÄ README.md                     # Documentaci√≥n principal
```

### Responsabilidades de Cada M√≥dulo

| M√≥dulo | Responsabilidad |
|--------|-----------------|
| `server.py` | Inicializa el servidor TCP y acepta conexiones |
| `client_handler.py` | Maneja el ciclo de vida de cada cliente conectado |
| `llm_client.py` | Se comunica con la API del modelo de lenguaje |
| `rate_limiter.py` | Implementa ventana deslizante para rate limiting |
| `ws_gateway.py` | Traduce entre WebSocket y TCP |
| `client_web.html` | Interface de usuario en el navegador |
| `config.py` | Centraliza configuraci√≥n usando `pydantic-settings` |

---

## Funcionamiento Paso a Paso

### 1. Inicio del Servidor

```python
# app/server.py
async def main():
    server = await asyncio.start_server(
        handle_client,         # Callback para cada cliente
        host=settings.APP_HOST,
        port=settings.APP_PORT,
    )
    async with server:
        await server.serve_forever()
```

**¬øQu√© ocurre aqu√≠?**
- Se crea un **socket de escucha** TCP en el puerto 5001
- El socket se configura como **no bloqueante**
- Por cada conexi√≥n entrante, `asyncio` **agenda autom√°ticamente** una nueva coroutine `handle_client()`
- El servidor queda en un loop infinito esperando conexiones

### 2. Llegada de un Cliente

```python
# app/client_handler.py
async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter):
    peer = writer.get_extra_info("peername")  # IP y puerto del cliente
    user = f"Usuario-{next(USER_SEQ)}"        # Etiqueta √∫nica
    
    # Saludo inicial
    greeting = f"{user} conectado.\nBienvenido a PsicoIA.\n..."
    writer.write(greeting.encode("utf-8"))
    await writer.drain()
```

**¬øQu√© ocurre aqu√≠?**
- Se obtiene la informaci√≥n del cliente (IP, puerto)
- Se asigna un **identificador √∫nico** (`Usuario-1`, `Usuario-2`, etc.)
- Se crea un **rate limiter exclusivo** para este cliente
- Se env√≠a un mensaje de bienvenida

### 3. Procesamiento de Mensajes

```python
while True:
    data = await reader.readline()  # Lee hasta \n, NO BLOQUEA
    if not data:
        break  # Cliente desconectado
    
    msg = data.decode().strip()
    
    # Verificar rate limit
    if not limiter.allow():
        writer.write("Demasiados mensajes...\n".encode("utf-8"))
        await writer.drain()
        continue
    
    # Sem√°foro global para limitar concurrencia
    async with SEM_GLOBAL:
        trace_id = f"{user}:m{next(msg_counter)}"
        
        # Llamada as√≠ncrona al LLM
        llm_reply = await llm_generate(msg, trace_id=trace_id, conversation_id=user)
        
        # Enviar respuesta al cliente
        writer.write((llm_reply + "\n").encode("utf-8"))
        await writer.drain()
```

**¬øQu√© ocurre aqu√≠?**
1. **Lectura as√≠ncrona**: `await reader.readline()` espera datos sin bloquear otros clientes
2. **Rate limiting**: Verifica que el usuario no est√© enviando demasiados mensajes
3. **Sem√°foro global**: Limita cu√°ntas llamadas simult√°neas al LLM pueden haber
4. **Trazabilidad**: Cada request tiene un `trace_id` √∫nico (ej: `Usuario-3:m5`)
5. **Llamada al LLM**: Se env√≠a el mensaje al modelo de lenguaje
6. **Respuesta**: Se escribe de vuelta al socket del cliente

### 4. Integraci√≥n con el LLM

```python
# app/services/llm_client.py
async def llm_generate(user_text: str, trace_id: str, conversation_id: str):
    # Recuperar historial de la conversaci√≥n
    history = await get_history(conversation_id)
    
    # Construir contexto con ventana deslizante
    messages = build_messages(SYSTEM_PROMPT, history, user_text)
    
    # Guardar turno del usuario
    await append_user(conversation_id, user_text)
    
    # Llamada HTTP as√≠ncrona a Groq
    async with httpx.AsyncClient(timeout=30) as client:
        resp = await client.post(url, headers=headers, json=payload)
        data = resp.json()
        content = data["choices"][0]["message"]["content"]
    
    # Guardar respuesta del asistente
    await append_assistant(conversation_id, content)
    
    return content
```

**¬øQu√© ocurre aqu√≠?**
1. **Recuperaci√≥n de historial**: Obtiene mensajes previos de este usuario
2. **Construcci√≥n del prompt**: Incluye contexto relevante dentro del presupuesto de tokens
3. **Llamada HTTP as√≠ncrona**: Usa `httpx.AsyncClient` para no bloquear
4. **Persistencia en RAM**: Guarda el intercambio en memoria para futuras referencias

### 5. Gateway WebSocket ‚Üî TCP

```python
# gateway/ws_gateway.py
async def bridge_ws_to_tcp(websocket):
    # Abrir conexi√≥n TCP al servidor
    reader, writer = await asyncio.open_connection(TCP_HOST, TCP_PORT)
    
    async def ws_reader():
        # Lee del WebSocket y escribe al TCP
        async for message in websocket:
            writer.write((message.strip() + "\n").encode("utf-8"))
            await writer.drain()
    
    async def tcp_reader():
        # Lee del TCP y escribe al WebSocket
        while not reader.at_eof():
            line = await reader.readline()
            if not line:
                break
            await websocket.send(line.decode("utf-8"))
    
    # Ejecutar ambas tareas en paralelo
    await asyncio.gather(ws_reader(), tcp_reader())
```

**¬øQu√© ocurre aqu√≠?**
1. Por cada conexi√≥n WebSocket, se abre **una conexi√≥n TCP propia** al servidor
2. Se crean **dos tareas as√≠ncronas** que corren en paralelo:
   - Una lee del WebSocket y escribe al TCP
   - Otra lee del TCP y escribe al WebSocket
3. Esto crea un **puente bidireccional** transparente
4. Cuando cualquiera de las dos conexiones se cierra, ambas se terminan

---

## Conceptos T√©cnicos Fundamentales

### üîå ¬øQu√© es un Socket?

Un **socket** es un punto final de comunicaci√≥n entre dos programas a trav√©s de una red. Piensa en √©l como un "enchufe virtual" donde se conectan dos aplicaciones para intercambiar datos.

**Analog√≠a**: Es como un n√∫mero de tel√©fono. El servidor tiene un n√∫mero (IP + puerto) y "atiende llamadas" de clientes que quieren comunicarse.

#### En el C√≥digo

```python
# El servidor crea un socket de escucha
server = await asyncio.start_server(
    handle_client,
    host="0.0.0.0",  # Escucha en todas las interfaces
    port=5001        # Puerto TCP
)

# Para cada cliente, asyncio crea autom√°ticamente:
# - reader: StreamReader (para leer del socket)
# - writer: StreamWriter (para escribir al socket)
```

**¬øD√≥nde est√° en nuestro proyecto?**
- `app/server.py`: Crea el socket de escucha
- `app/client_handler.py`: Usa `reader` y `writer` para cada cliente
- `gateway/ws_gateway.py`: Abre sockets TCP hacia el servidor

### üåê ¬øQu√© es TCP?

**TCP (Transmission Control Protocol)** es un protocolo de comunicaci√≥n que garantiza:

1. **Confiabilidad**: Los datos llegan o se notifica el error
2. **Orden**: Los mensajes llegan en el orden enviado
3. **Control de flujo**: No se sobrecarga al receptor
4. **Detecci√≥n de errores**: Checksums para validar integridad

#### ¬øPor qu√© los mensajes llegan en orden?

TCP implementa **n√∫meros de secuencia**:
- Cada byte enviado tiene un n√∫mero √∫nico
- El receptor reordena paquetes si llegan desordenados
- Reconoce la recepci√≥n con ACKs (acknowledgments)
- Retransmite paquetes perdidos

```
Cliente env√≠a:  [Paq1: "Hola"] [Paq2: " mundo"]
Red desordena:  [Paq2] llega primero
TCP reordena:   [Paq1] [Paq2]
App recibe:     "Hola mundo"  ‚Üê ¬°En orden!
```

**Ventaja para nuestro proyecto**: No necesitamos preocuparnos por reordenamiento de mensajes. TCP lo maneja autom√°ticamente.

### üîÑ ¬øQu√© es Asincronismo?

**Asincronismo** es un paradigma de programaci√≥n donde las operaciones que tardan (I/O de red, disco, etc.) **no bloquean** la ejecuci√≥n del programa.

#### Comparaci√≥n: S√≠ncrono vs As√≠ncrono

**C√≥digo S√≠ncrono (Bloqueante)**:
```python
data = socket.read()  # ‚è∏Ô∏è Bloquea hasta que lleguen datos
# No puede hacer nada m√°s mientras espera
process(data)
```

**C√≥digo As√≠ncrono (No Bloqueante)**:
```python
data = await reader.readline()  # üèÉ Cede el control mientras espera
# El event loop puede atender otras conexiones
process(data)
```

#### Event Loop

El **event loop** es el coraz√≥n de `asyncio`. Es un bucle que:
1. Revisa qu√© tareas est√°n esperando I/O
2. Ejecuta las que tienen datos disponibles
3. Registra callbacks para las que todav√≠a esperan
4. Vuelve a empezar

```python
# Visualizaci√≥n conceptual del event loop
while True:
    tareas_listas = obtener_tareas_con_datos_disponibles()
    for tarea in tareas_listas:
        ejecutar_hasta_proximo_await(tarea)
    
    registrar_callbacks_para_IO_pendiente()
    esperar_eventos_IO_del_OS()
```

**En nuestro proyecto**:
```python
# M√∫ltiples clientes pueden estar en diferentes estados
Cliente 1: await reader.readline()     # Esperando entrada
Cliente 2: await llm_generate(...)     # Esperando respuesta del LLM
Cliente 3: await writer.drain()        # Esperando que se env√≠en datos

# El event loop alterna entre ellos eficientemente
```

### üßµ ¬øQu√© es un Hilo (Thread)?

Un **hilo** es una unidad de ejecuci√≥n dentro de un proceso. Permite ejecutar m√∫ltiples flujos de c√≥digo "simult√°neamente" (en realidad se turnan muy r√°pido).

**Problema con hilos**:
- Cada hilo consume ~8MB de RAM (stack)
- Context switch (cambio de hilo) es costoso
- Sincronizaci√≥n compleja (locks, race conditions)

```python
# Ejemplo con hilos (NO usado en nuestro proyecto)
import threading

def handle_client(socket):
    # Maneja un cliente
    pass

for connection in connections:
    thread = threading.Thread(target=handle_client, args=(connection,))
    thread.start()

# Con 1000 clientes = 1000 hilos = ~8GB RAM solo en stacks
```

### ‚ö° ¬øQu√© es Concurrencia?

**Concurrencia** es la capacidad de manejar m√∫ltiples tareas progresando al mismo tiempo (aunque no necesariamente ejecut√°ndose en el mismo instante).

**No es lo mismo que paralelismo**:
- **Concurrencia**: Dos tareas progresan intercal√°ndose (un solo n√∫cleo CPU)
- **Paralelismo**: Dos tareas ejecut√°ndose realmente al mismo tiempo (m√∫ltiples n√∫cleos CPU)

**Nuestro proyecto usa concurrencia cooperativa**:
```python
# M√∫ltiples clientes comparten un solo hilo
async def handle_client(reader, writer):
    while True:
        data = await reader.readline()  # Cede control aqu√≠
        # Cuando hay datos, retoma desde aqu√≠
        response = await llm_generate(data)  # Cede control aqu√≠
        await writer.drain()  # Cede control aqu√≠
```

### üö¶ Nivel de Concurrencia

El **nivel de concurrencia** determina cu√°ntas operaciones simult√°neas permite el sistema.

**En nuestro proyecto**:

1. **Conexiones simult√°neas**: Ilimitadas (limitado solo por recursos del SO)
   ```python
   # Cada cliente tiene su propia coroutine
   server = await asyncio.start_server(handle_client, ...)
   ```

2. **Llamadas simult√°neas al LLM**: Limitadas por sem√°foro
   ```python
   SEM_GLOBAL = asyncio.Semaphore(settings.MAX_IN_FLIGHT)
   
   async with SEM_GLOBAL:  # Solo N pueden entrar aqu√≠ simult√°neamente
       llm_reply = await llm_generate(...)
   ```

3. **Mensajes por usuario**: Limitados por rate limiter
   ```python
   limiter = SlidingWindowLimiter(max_messages=10, window_seconds=60)
   if not limiter.allow():
       return "Demasiados mensajes"
   ```

**Control de flujo**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1000 clientes conectados ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îú‚îÄ‚îÄ‚ñ∫ 995 esperando datos (await readline)
       ‚îÇ
       ‚îú‚îÄ‚îÄ‚ñ∫ 3 esperando respuesta LLM (await llm_generate)
       ‚îÇ           ‚îÇ
       ‚îÇ           ‚îú‚îÄ‚ñ∫ Solo 2 en vuelo (SEM_GLOBAL = 2)
       ‚îÇ           ‚îî‚îÄ‚ñ∫ 1 esperando en sem√°foro
       ‚îÇ
       ‚îî‚îÄ‚îÄ‚ñ∫ 2 enviando respuesta (await drain)
```

### üîç ¬øC√≥mo Identificamos Usuarios?

**Estrategia de identificaci√≥n**:
1. **Contador secuencial**: `Usuario-1`, `Usuario-2`, etc.
2. **Por conexi√≥n**: Cada socket TCP es √∫nico
3. **Historial por conversation_id**: Se usa el identificador del usuario como clave

```python
# En client_handler.py
USER_SEQ = itertools.count(1)  # Contador global at√≥mico

async def handle_client(reader, writer):
    user = f"Usuario-{next(USER_SEQ)}"  # At√≥mico por naturaleza de asyncio
    
    # Cada cliente tiene su propio:
    # - Socket (reader/writer)
    # - Rate limiter
    # - Contador de mensajes
    # - Historial conversacional
```

**Aislamiento de estado**:
```python
# Historial en RAM por conversation_id
_histories: dict[str, list[dict]] = defaultdict(list)

# "Usuario-1" ‚Üí [{"role": "user", "content": "Hola"}, ...]
# "Usuario-2" ‚Üí [{"role": "user", "content": "Ayuda"}, ...]
# No se mezclan porque cada uno tiene su clave √∫nica
```

---

## Decisiones Tecnol√≥gicas y Justificaciones

### ‚úÖ Por qu√© AsyncIO en vez de Threading

| Aspecto | Threading | AsyncIO |
|---------|-----------|---------|
| **Escalabilidad** | ~100-1000 hilos | ~10,000-100,000 coroutines |
| **Memoria** | 8MB por hilo | ~1KB por coroutine |
| **Context switch** | Costoso (kernel) | Barato (user space) |
| **Sincronizaci√≥n** | Locks, mutexes | Naturalmente cooperativo |
| **Ideal para** | CPU-bound | I/O-bound (nuestro caso) |
| **GIL** | Contenci√≥n del GIL | Sin problema (un solo thread) |

#### Razones Clave:

**1. Menor sobrecarga (Overhead)**
```python
# Threading: 1000 clientes = 1000 hilos
# - 8MB √ó 1000 = 8GB RAM solo en stacks
# - Context switches constantes (kernel space)
# - Contenci√≥n del GIL de Python

# AsyncIO: 1000 clientes = 1000 coroutines
# - ~1KB √ó 1000 = ~10MB RAM (coroutines + buffers)
# - Context switches cooperativos (user space)
# - Sin GIL (un solo thread, no hay competencia)
```

**2. Mejor control con Event Loop expl√≠cito**
```python
# Con threading: el SO decide cu√°ndo cambiar de hilo (preemptivo)
# Con asyncio: el c√≥digo decide cu√°ndo ceder control (cooperativo)

async def handle_client(reader, writer):
    data = await reader.readline()  # Cede control aqu√≠
    # Solo retoma cuando hay datos disponibles
    response = await llm_generate(data)  # Cede control aqu√≠
    await writer.drain()  # Cede control aqu√≠
```

**3. Ideal para I/O-bound (operaciones de red)**
```python
# Nuestro tiempo de procesamiento:
# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
# ‚îÇ Esperando I/O (red): 95%        ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
# ‚îÇ Procesando CPU: 5%              ‚îÇ‚ñà
# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

# Threading ser√≠a desperdicio:
# - Hilos esperan bloqueados (no hacen nada √∫til)
# - Overhead de sincronizaci√≥n para casi nada de CPU

# AsyncIO es perfecto:
# - Cuando un cliente espera I/O, otro progresa
# - CPU siempre ocupada con trabajo √∫til
# - Sin overhead de cambio de contexto del kernel
```

**4. Sin contenci√≥n del GIL (Global Interpreter Lock)**
```python
# GIL de Python: solo un thread ejecuta bytecode a la vez

# Con threading:
Thread 1: ‚îÄ‚îÄ[GIL]‚îÄ‚îÄ[espera]‚îÄ‚îÄ[GIL]‚îÄ‚îÄ[espera]‚îÄ‚îÄ
Thread 2: ‚îÄ‚îÄ[espera]‚îÄ‚îÄ[GIL]‚îÄ‚îÄ[espera]‚îÄ‚îÄ[GIL]‚îÄ‚îÄ
# ‚Üí Competencia por el GIL incluso cuando esperan I/O

# Con asyncio:
Thread √∫nico: ‚îÄ‚îÄ[trabaja]‚îÄ‚îÄ[espera I/O]‚îÄ‚îÄ[trabaja]‚îÄ‚îÄ
# ‚Üí No hay competencia, un solo thread, no hay GIL problem
```

**Justificaci√≥n**:
Nuestro servidor es **I/O bound** (red, llamadas HTTP). La mayor√≠a del tiempo se gasta esperando:
- Datos del socket del cliente ‚Üí `await reader.readline()`
- Respuesta del LLM (HTTP) ‚Üí `await client.post()`
- Escritura al socket ‚Üí `await writer.drain()`

AsyncIO permite **miles de conexiones simult√°neas** con bajo overhead de memoria y CPU, y sin los problemas del GIL de Python.

### ‚úÖ Por qu√© Separar Servidor TCP y Gateway WebSocket

**Arquitectura de Gateway** (patr√≥n de dise√±o Adapter/Bridge):

#### Razones Clave:

**1. Separaci√≥n de responsabilidades (SRP - Single Responsibility Principle)**
```python
# Gateway (gateway/ws_gateway.py)
# - Solo maneja: WebSocket ‚Üî TCP
# - No sabe: L√≥gica de negocio, LLM, rate limiting
# - Responsabilidad: Traducci√≥n de protocolos

# Servidor (app/server.py + client_handler.py)
# - Solo maneja: L√≥gica de negocio, LLM, rate limiting
# - No sabe: WebSocket, HTTP, detalles de transporte
# - Responsabilidad: Procesamiento de mensajes
```

**2. Simplicidad de testing**
```bash
# Probar servidor TCP directamente (sin gateway)
telnet localhost 5001
> Hola
< Usuario-1 conectado. Bienvenido a PsicoIA...

# Probar con netcat
nc localhost 5001
> ¬øC√≥mo est√°s?
< [Respuesta del LLM]

# Sin necesidad de:
# - Abrir navegador
# - Configurar WebSocket
# - Cliente JavaScript
```

**3. Flexibilidad de protocolos**
```python
# Arquitectura extensible sin tocar el core:

HTTP REST API ‚îÄ‚îÄ‚îê
                ‚îÇ
WebSocket ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚Üí [Gateway Layer] ‚îÄ‚îÄ‚Üí [TCP Server]
                ‚îÇ
gRPC ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                ‚îÇ
MQTT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

# Cada gateway es independiente:
# - gateway/ws_gateway.py    (actual)
# - gateway/http_gateway.py  (futuro)
# - gateway/grpc_gateway.py  (futuro)

# Servidor TCP NO CAMBIA
```

**4. Aislamiento de sesi√≥n (1:1 mapping)**
```python
# gateway/ws_gateway.py
async def bridge_ws_to_tcp(websocket):
    # Cada conexi√≥n WebSocket abre SU PROPIA conexi√≥n TCP
    reader, writer = await asyncio.open_connection(TCP_HOST, TCP_PORT)
    
    # Mapeo 1:1 garantizado:
    # WebSocket Cliente A ‚Üê‚Üí TCP Conexi√≥n A ‚Üê‚Üí Usuario-1
    # WebSocket Cliente B ‚Üê‚Üí TCP Conexi√≥n B ‚Üê‚Üí Usuario-2
    
    # ‚úÖ No hay contaminaci√≥n de estado entre usuarios
    # ‚úÖ No hay mezcla de mensajes
    # ‚úÖ Cada usuario tiene su propio canal aislado
```

**5. Ventajas adicionales**:
- ‚úÖ **Testeable**: Cada componente se prueba independientemente
- ‚úÖ **Extensible**: Agregar protocolos sin modificar l√≥gica de negocio
- ‚úÖ **Mantenible**: Bug en gateway no afecta al servidor TCP
- ‚úÖ **Escalable**: Gateway y servidor pueden estar en m√°quinas diferentes
- ‚úÖ **Reusable**: El servidor TCP puede ser usado por otros clientes

**Alternativa (monolito) - NO recomendado**:
```python
# ‚ùå Todo en un solo servidor
async def handle_connection(connection):
    if isinstance(connection, WebSocketConnection):
        # L√≥gica WebSocket aqu√≠
        await handle_websocket(connection)
    elif isinstance(connection, TCPConnection):
        # L√≥gica TCP aqu√≠
        await handle_tcp(connection)
    
    # Problemas:
    # - C√≥digo complejo y acoplado
    # - Dif√≠cil de testear
    # - Cambios en un protocolo afectan a otros
    # - Violaci√≥n del SRP
```

**Flujo de datos**:
```
Navegador ‚Üí [WebSocket] ‚Üí Gateway ‚Üí [TCP] ‚Üí Servidor ‚Üí LLM
         ‚Üê             ‚Üê          ‚Üê       ‚Üê          ‚Üê
         
Cliente nativo ‚Üí [TCP directo] ‚Üí Servidor ‚Üí LLM
              ‚Üê                ‚Üê          ‚Üê
```

### ‚úÖ Por qu√© Almacenar Historial en RAM y No en Base de Datos

**Decisi√≥n para prototipo**:

| Opci√≥n | Ventajas | Desventajas |
|--------|----------|-------------|
| **RAM** | - Simple<br>- R√°pido (< 1ms)<br>- Sin dependencias<br>- Sin configuraci√≥n | - No persistente<br>- Pierde datos al reiniciar<br>- Limitado por memoria |
| **DB** | - Persistente<br>- Escalable<br>- Consultas complejas | - Complejidad (setup, migraciones)<br>- Latencia adicional (5-50ms)<br>- Dependencia externa |

#### Razones Clave:

**1. Prototipado r√°pido**
```python
# Implementaci√≥n en RAM: ~20 l√≠neas
_histories: dict[str, list[dict]] = defaultdict(list)

async def get_history(conversation_id: str) -> list[dict]:
    return list(_histories.get(conversation_id, []))

async def append_user(conversation_id: str, text: str):
    _histories[conversation_id].append({"role": "user", "content": text})

# Implementaci√≥n con DB: ~200+ l√≠neas
# - Configurar conexi√≥n async (asyncpg/motor)
# - Crear tablas (migrations)
# - Manejo de transacciones
# - Pool de conexiones
# - Manejo de errores de red
# - Reintentos, timeouts
```

**2. Suficiente para demostraci√≥n**
```python
# Para pruebas y demos:
# ‚úÖ 100-1000 usuarios concurrentes: OK
# ‚úÖ Historial de 50-200 mensajes por usuario: OK
# ‚úÖ Memoria: ~1MB por 1000 mensajes
# ‚úÖ Latencia: < 1ms (acceso a dict)

# Para producci√≥n:
# ‚ùå Miles de usuarios persistentes
# ‚ùå Historial de a√±os
# ‚ùå An√°lisis de datos hist√≥ricos
# ‚Üí Entonces s√≠ necesitamos DB
```

**3. Ruta de migraci√≥n clara (interfaz definida)**
```python
# Interfaz actual (abstracta del almacenamiento):
async def get_history(conversation_id: str) -> list[dict]
async def append_user(conversation_id: str, text: str) -> None
async def append_assistant(conversation_id: str, text: str) -> None

# Migraci√≥n futura: cambiar implementaci√≥n, no la interfaz
async def get_history(conversation_id: str) -> list[dict]:
    # Versi√≥n RAM (actual):
    return list(_histories.get(conversation_id, []))
    
    # Versi√≥n PostgreSQL (futuro):
    # async with db_pool.acquire() as conn:
    #     rows = await conn.fetch(
    #         "SELECT role, content FROM messages "
    #         "WHERE user_id = $1 ORDER BY timestamp",
    #         conversation_id
    #     )
    #     return [dict(row) for row in rows]

# ‚úÖ C√≥digo cliente NO CAMBIA
# ‚úÖ Solo cambiar llm_client.py
```

**4. Protecci√≥n con Locks (evita race conditions)**
```python
# Sin locks (‚ùå race condition):
_histories[user] = [...]
# Thread A: lee lista
# Thread B: modifica lista
# Thread A: escribe lista (sobrescribe cambios de B)

# Con locks (‚úÖ thread-safe):
_locks: dict[str, asyncio.Lock] = {}

async def append_user(conversation_id: str, text: str):
    async with _get_lock(conversation_id):
        _histories[conversation_id].append(
            {"role": "user", "content": text}
        )
    # ‚úÖ Solo una coroutine modifica el historial a la vez
    # ‚úÖ Garant√≠a de atomicidad
```

**5. Sin dependencias externas**
```bash
# RAM: no requiere
docker-compose up  # ‚úÖ Funciona inmediatamente

# Con DB: requiere
# - PostgreSQL/MongoDB container
# - Schema/migrations
# - Configuraci√≥n de credenciales
# - Manejo de conexiones
# - ¬øDB no disponible? ‚Üí Sistema no funciona
```

**Justificaci√≥n**:
1. **Objetivo educativo**: Prioridad es entender sockets, asyncio, concurrencia (no administraci√≥n de DB)
2. **Prototipo funcional r√°pido**: De la idea a c√≥digo funcional en horas, no d√≠as
3. **Suficiente para el alcance**: Demo universitaria con 10-20 usuarios simult√°neos
4. **Migraci√≥n preparada**: La interfaz permite cambiar el backend sin tocar la l√≥gica de negocio

**Performance comparado**:
```python
# Acceso a RAM:
await get_history("Usuario-1")  # < 0.1ms

# Acceso a PostgreSQL local:
await db.fetch("SELECT ...")     # 5-10ms

# Acceso a PostgreSQL remoto:
await db.fetch("SELECT ...")     # 20-50ms

# Para nuestra app: la diferencia es insignificante
# Para 10,000 req/s: la diferencia es cr√≠tica ‚Üí necesitamos Redis
```

### ‚úÖ Por qu√© Docker Compose

**Beneficios de la containerizaci√≥n**:

#### Razones Clave:

**1. Reproducibilidad del entorno**
```bash
# Desarrollador A (Windows 11, Python 3.11)
docker-compose up
# ‚Üí Usa Python 3.12 del contenedor

# Desarrollador B (Mac M1, Python 3.9)
docker-compose up
# ‚Üí Usa Python 3.12 del contenedor

# Servidor producci√≥n (Ubuntu 22.04)
docker-compose up
# ‚Üí Usa Python 3.12 del contenedor

# ‚úÖ Mismo comportamiento en todas las m√°quinas
# ‚úÖ "Funciona en mi m√°quina" ‚Üí "Funciona en TODAS las m√°quinas"
```

**2. Orquestaci√≥n autom√°tica de servicios**
```yaml
# docker-compose.yml
services:
  app:
    build: .
    ports: ["5001:5001"]
    
  gateway:
    image: psicoia:latest  # Reutiliza imagen de 'app'
    ports: ["8765:8765"]
    environment:
      - TCP_HOST=app  # Nombre del servicio = hostname DNS
    depends_on:
      app:
        condition: service_started  # Espera a que app est√© listo

# ‚úÖ Docker Compose maneja:
# - Orden de inicio (app antes que gateway)
# - Red interna autom√°tica
# - DNS entre servicios (gateway puede hacer `connect('app', 5001)`)
# - Variables de entorno
# - Reinicio autom√°tico si se cae
```

**3. Aislamiento completo**
```python
# Sin Docker:
# - pip install en sistema global
# - Conflictos de versiones (httpx 0.24 vs 0.27)
# - Variables de entorno del usuario
# - Puerto 5001 puede estar ocupado

# Con Docker:
# ‚úÖ Cada contenedor tiene su propio:
#    - Sistema de archivos
#    - Espacio de procesos
#    - Network namespace (puertos)
#    - Variables de entorno
# ‚úÖ No contamina el sistema host
# ‚úÖ No hay conflictos
```

**4. Despliegue sencillo (un solo comando)**
```bash
# Sin Docker (setup manual):
# 1. Instalar Python 3.12
sudo apt install python3.12
# 2. Crear virtualenv
python3.12 -m venv venv
source venv/bin/activate
# 3. Instalar dependencias
pip install -r requirements.txt
# 4. Configurar .env
cp .env.example .env
vim .env  # Editar manualmente
# 5. Abrir 2 terminales
# Terminal 1:
python -m app.server
# Terminal 2:
python -m gateway.ws_gateway
# ‚Üí 5 pasos, propenso a errores

# Con Docker (setup autom√°tico):
docker-compose up -d
# ‚Üí 1 comando, todo funciona
# ‚úÖ Dependencias instaladas autom√°ticamente
# ‚úÖ Ambos servicios corriendo
# ‚úÖ Configuraci√≥n desde .env
# ‚úÖ Redes configuradas
```

**5. Gesti√≥n de dependencias y servicios**
```yaml
# F√°cil agregar m√°s servicios:
services:
  app:
    build: .
  
  gateway:
    image: psicoia:latest
  
  # Futuro: agregar PostgreSQL
  db:
    image: postgres:15
    environment:
      POSTGRES_DB: psicoia
      POSTGRES_PASSWORD: secret
  
  # Futuro: agregar Redis
  redis:
    image: redis:7-alpine
  
  # Futuro: agregar monitoring
  prometheus:
    image: prom/prometheus

# docker-compose up ‚Üí Todos se levantan con sus dependencias
```

**Comparaci√≥n**:

| Aspecto | Sin Docker | Con Docker |
|---------|------------|------------|
| **Setup** | 5+ pasos manuales | 1 comando |
| **Reproducibilidad** | Depende del SO y versiones | 100% reproducible |
| **Aislamiento** | Sistema global | Contenedores aislados |
| **Portabilidad** | "Funciona en mi m√°quina" | Funciona en todas |
| **Dependencias** | Conflictos posibles | Totalmente aisladas |
| **Documentaci√≥n** | README largo | docker-compose.yml es la doc |
| **CI/CD** | Setup complejo | `docker build && docker push` |

**Comandos √∫tiles**:
```bash
# Levantar servicios
docker-compose up -d

# Ver logs en tiempo real
docker-compose logs -f app

# Reiniciar un servicio
docker-compose restart gateway

# Bajar todo
docker-compose down

# Reconstruir im√°genes
docker-compose build --no-cache

# Ver estado
docker-compose ps

# Entrar a un contenedor
docker-compose exec app bash
```

**Justificaci√≥n**:
Para un proyecto que debe ser **defendido y demostrado** ante un profesor, Docker Compose garantiza que:
- ‚úÖ Funciona en la m√°quina del profesor sin setup
- ‚úÖ No hay "pero en mi m√°quina funcionaba"
- ‚úÖ La demo es profesional y confiable
- ‚úÖ Demuestra conocimiento de DevOps moderno

---

## Observabilidad y Monitoreo

### Sistema de Logs

**Niveles de logging**:
```python
# app/utils/logger.py
log.debug("Detalle t√©cnico")      # Desarrollo
log.info("Evento importante")     # Producci√≥n
log.warning("Situaci√≥n at√≠pica")  # Alertas
log.error("Error recuperable")    # Errores
log.exception("Error cr√≠tico")    # Con stack trace
```

**Logs estructurados**:
```python
log.info(f"[{trace_id}] POST {url} model={model} len={len(user_text)}")
# [Usuario-3:m5] POST https://api.groq.com/... model=llama-3.1 len=42

log.info(f"[{trace_id}] ‚Üê LLM ok ({len(llm_reply)} chars) {dt_ms:.0f} ms")
# [Usuario-3:m5] ‚Üê LLM ok (156 chars) 847 ms
```

### Trazabilidad (Trace ID)

**¬øQu√© es un trace_id?**
Un identificador √∫nico que vincula todas las operaciones de una request.

**Formato**: `Usuario-N:mM`
- `Usuario-N`: Cliente espec√≠fico
- `mM`: N√∫mero de mensaje de ese cliente

**Ejemplo de flujo completo**:
```
[Usuario-3] Conexi√≥n desde ('192.168.1.100', 54321)
[Usuario-3:m1] ‚Üí LLM start (len=42)
[Usuario-3:m1] POST https://api.groq.com/openai/v1/chat/completions
[Usuario-3:m1] ‚Üê LLM ok (156 chars) 847 ms
[Usuario-3:m2] ‚Üí LLM start (len=58)
[Usuario-3:m2] ‚Üê LLM ok (203 chars) 1205 ms
[Usuario-3] Conexi√≥n cerrada
```

**Ventajas**:
- Correlacionar logs distribuidos
- Medir latencia por operaci√≥n
- Debugging facilitado
- An√°lisis de performance

### Rate Limiting

**Implementaci√≥n**: Ventana deslizante (Sliding Window)

```python
class SlidingWindowLimiter:
    def __init__(self, max_events: int, window_seconds: int):
        self.max = max_events
        self.win = window_seconds
        self.events = deque()  # Timestamps de eventos
    
    def allow(self) -> bool:
        now = time.monotonic()
        
        # Eliminar eventos fuera de la ventana
        while self.events and now - self.events[0] > self.win:
            self.events.popleft()
        
        # Verificar si hay espacio
        if len(self.events) < self.max:
            self.events.append(now)
            return True
        return False
```

**Ejemplo de uso**:
```
Configuraci√≥n: 10 mensajes por 60 segundos

Mensajes del usuario:
t=0s:  msg1 ‚úÖ (1/10)
t=1s:  msg2 ‚úÖ (2/10)
...
t=9s:  msg10 ‚úÖ (10/10)
t=10s: msg11 ‚ùå Bloqueado
t=61s: msg12 ‚úÖ (msg1 sali√≥ de la ventana)
```

### M√©tricas Clave

**Disponibles en los logs**:
1. **Latencia del LLM**: Tiempo de respuesta del modelo
2. **Conexiones activas**: N√∫mero de usuarios simult√°neos
3. **Rate limiting**: Cu√°ntos requests se bloquean
4. **Errores**: Tasa de fallos del LLM

**Ejemplo de log agregado**:
```
[INFO] TCP server escuchando en ('0.0.0.0', 5001)
[INFO] [Usuario-1] Conexi√≥n desde ('127.0.0.1', 52847)
[INFO] [Usuario-1:m1] ‚Üí LLM start (len=45)
[INFO] [Usuario-1:m1] ‚Üê LLM ok (198 chars) 923 ms
[WARNING] [Usuario-2:m15] Rate limit excedido
[ERROR] [Usuario-3:m8] Groq error: 429 Rate Limit
```

---

## Docker y Containerizaci√≥n

### Dockerfile

```dockerfile
FROM python:3.12-slim

WORKDIR /app

# Instalar dependencias del sistema
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential pkg-config && \
    rm -rf /var/lib/apt/lists/*

# Copiar archivo de dependencias
COPY requirements.txt ./

# Instalar dependencias Python
RUN pip install --no-cache-dir -U pip && \
    pip install --no-cache-dir -r requirements.txt

# Copiar c√≥digo de la aplicaci√≥n
COPY . .

# Exponer puertos
EXPOSE 5001 8765
```

**Capas de la imagen**:
1. **Base**: Python 3.12 slim (~45MB)
2. **Build tools**: Compiladores para librer√≠as nativas
3. **Dependencias Python**: asyncio, httpx, websockets, etc.
4. **C√≥digo aplicaci√≥n**: Nuestros archivos .py

### Docker Compose

```yaml
services:
  app:
    build: .
    image: psicoia:latest
    container_name: psicoia_app
    env_file: .env
    command: ["python", "-m", "app.server"]
    ports:
      - "${APP_PORT:-5001}:5001"

  gateway:
    image: psicoia:latest  # Reutiliza la misma imagen
    container_name: psicoia_gateway
    env_file: .env
    environment:
      - TCP_HOST=app  # Nombre del servicio como hostname
      - TCP_PORT=5001
      - WS_HOST=0.0.0.0
      - WS_PORT=8765
    command: ["python", "-m", "gateway.ws_gateway"]
    ports:
      - "8765:8765"
    depends_on:
      app:
        condition: service_started
```

**¬øC√≥mo se comunican los contenedores?**

Docker Compose crea una **red privada** donde:
- Cada contenedor tiene su hostname = nombre del servicio
- `gateway` puede acceder a `app` usando `TCP_HOST=app`
- La resoluci√≥n DNS es autom√°tica

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Red Docker "psicoia_default"   ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   app    ‚îÇ    ‚îÇ  gateway  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  :5001   ‚îÇ‚Üê‚îÄ‚îÄ‚îÄ‚îÇ  :8765    ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ        ‚Üë              ‚Üë         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ              ‚îÇ
      Puerto        Puerto
      5001          8765
```

### Comandos Docker √ötiles

```bash
# Construir y levantar servicios
docker-compose up -d

# Ver logs en tiempo real
docker-compose logs -f

# Ver logs de un servicio espec√≠fico
docker-compose logs -f app

# Reiniciar servicios
docker-compose restart

# Detener servicios
docker-compose down

# Reconstruir imagen
docker-compose build --no-cache

# Ver contenedores corriendo
docker-compose ps

# Entrar a un contenedor
docker-compose exec app bash
```

### Simulaci√≥n de Condiciones de Red

**Para demostrar robustez** puedes simular:

1. **Latencia de red**:
   ```bash
   # En Linux (dentro del contenedor)
   tc qdisc add dev eth0 root netem delay 100ms
   ```

2. **P√©rdida de paquetes**:
   ```bash
   tc qdisc add dev eth0 root netem loss 5%
   ```

3. **L√≠mite de ancho de banda**:
   ```bash
   tc qdisc add dev eth0 root tbf rate 1mbit burst 32kbit latency 400ms
   ```

4. **Desconexi√≥n abrupta**:
   ```bash
   docker-compose stop app
   # El gateway manejar√° el error y notificar√° al cliente
   ```

**Nuestro sistema es resiliente porque**:
- **Timeouts configurables**: `LLM_TIMEOUT_SECONDS`
- **Reintentos autom√°ticos**: `LLM_MAX_RETRIES`
- **Backoff exponencial**: Espera cada vez m√°s entre reintentos
- **Manejo de errores**: Notifica al usuario si algo falla

---

## Posibles Preguntas del Profesor

### 1. ¬øC√≥mo funciona el servidor?

**Respuesta**:

El servidor es un **servidor TCP as√≠ncrono** basado en `asyncio`. Funciona as√≠:

1. Se crea un **socket de escucha** en el puerto 5001
2. El socket acepta conexiones entrantes de forma **no bloqueante**
3. Por cada cliente que se conecta, se crea autom√°ticamente una **coroutine independiente** (`handle_client`)
4. Cada coroutine maneja el ciclo de vida completo de su cliente: recibir mensajes, procesar con el LLM, enviar respuestas
5. El **event loop** de asyncio coordina todas las coroutines, alternando entre ellas cuando esperan I/O

**C√≥digo clave**:
```python
server = await asyncio.start_server(handle_client, host="0.0.0.0", port=5001)
async with server:
    await server.serve_forever()  # Loop infinito
```

---

### 2. ¬øC√≥mo gestionan m√∫ltiples clientes?

**Respuesta**:

Usamos **concurrencia cooperativa** con `asyncio`:

1. **Una coroutine por cliente**: Cada conexi√≥n TCP tiene su propia coroutine `handle_client()`
2. **Aislamiento de estado**: Cada cliente tiene su propio:
   - Reader/Writer (socket)
   - Rate limiter
   - Historial conversacional
   - Identificador √∫nico (`Usuario-N`)

3. **No hay bloqueos**: Cuando un cliente espera datos (`await reader.readline()`), el event loop atiende a otros

**Capacidad**: Podemos manejar **miles de clientes** simult√°neamente con un solo hilo, porque:
- No usamos hilos pesados
- Las coroutines son livianas (~1KB cada una)
- La mayor parte del tiempo se espera I/O (no se usa CPU)

---

### 3. ¬øC√≥mo manejan la concurrencia?

**Respuesta**:

Implementamos **m√∫ltiples niveles de control de concurrencia**:

#### Nivel 1: Conexiones simult√°neas (ilimitadas)
```python
# Asyncio maneja autom√°ticamente miles de conexiones
server = await asyncio.start_server(...)
```

#### Nivel 2: Rate limiting por usuario
```python
# Cada cliente tiene un rate limiter que previene flooding
limiter = SlidingWindowLimiter(max_messages=10, window_seconds=60)
if not limiter.allow():
    return "Demasiados mensajes"
```

#### Nivel 3: Sem√°foro global para el LLM
```python
# Limita cu√°ntas llamadas simult√°neas al LLM pueden haber
SEM_GLOBAL = asyncio.Semaphore(MAX_IN_FLIGHT)
async with SEM_GLOBAL:
    llm_reply = await llm_generate(...)
```

**¬øPor qu√© tres niveles?**
- **Nivel 1**: Aceptamos todas las conexiones para no rechazar clientes
- **Nivel 2**: Prevenimos que un usuario malicioso sature el sistema
- **Nivel 3**: Protegemos el recurso compartido (API del LLM) contra sobrecarga

---

### 4. ¬øQu√© pasa si dos usuarios mandan mensajes al mismo tiempo?

**Respuesta**:

**No hay problema** porque cada usuario est√° completamente aislado:

1. **Sockets independientes**: Cada cliente tiene su propio par reader/writer
2. **Coroutines independientes**: No comparten variables (excepto estructuras thread-safe)
3. **Historial separado**: Usamos `conversation_id` √∫nico como clave en el diccionario

**Ejemplo**:
```python
# Cliente 1 env√≠a "Hola"
Usuario-1:m1 ‚Üí llm_generate("Hola", conversation_id="Usuario-1")
# Mientras espera respuesta del LLM...

# Cliente 2 env√≠a "Ayuda"
Usuario-2:m1 ‚Üí llm_generate("Ayuda", conversation_id="Usuario-2")
# Ambos progresan independientemente

# Historiales separados:
_histories["Usuario-1"] = [{"role": "user", "content": "Hola"}, ...]
_histories["Usuario-2"] = [{"role": "user", "content": "Ayuda"}, ...]
```

**Sincronizaci√≥n**: El √∫nico punto de sincronizaci√≥n es el **sem√°foro global**, que garantiza que no haya m√°s de N llamadas simult√°neas al LLM, pero no mezcla los datos de los usuarios.

---

### 5. ¬øQu√© pasar√≠a sin asyncio?

**Respuesta**:

Sin `asyncio`, tendr√≠amos que usar **threads o multiprocessing**, lo cual tiene serios inconvenientes:

#### Opci√≥n A: Un hilo por cliente
```python
import threading

def handle_client(socket):
    # C√≥digo bloqueante
    data = socket.recv(1024)  # Bloquea este hilo
    # ...

for connection in connections:
    thread = threading.Thread(target=handle_client, args=(connection,))
    thread.start()
```

**Problemas**:
- ‚ùå **Escalabilidad limitada**: ~100-1000 hilos m√°ximo
- ‚ùå **Consumo de memoria**: 8MB √ó 1000 hilos = 8GB RAM solo en stacks
- ‚ùå **Context switching costoso**: Cambios de hilo involucran al kernel
- ‚ùå **Sincronizaci√≥n compleja**: Necesitamos locks para el historial compartido
- ‚ùå **Race conditions**: F√°cil introducir bugs de concurrencia

#### Opci√≥n B: Blocking I/O secuencial
```python
# Un solo hilo atiende clientes uno a uno
while True:
    data = socket.recv(1024)  # Bloquea todo el servidor
    process(data)
```

**Problemas**:
- ‚ùå **Un solo cliente a la vez**: El segundo cliente espera que termine el primero
- ‚ùå **Latencia inaceptable**: Si el LLM tarda 3 segundos, todos esperan

#### Con asyncio (nuestra soluci√≥n):
- ‚úÖ **Miles de clientes** con bajo overhead
- ‚úÖ **Concurrencia cooperativa** sin race conditions
- ‚úÖ **C√≥digo claro** con sintaxis `async/await`
- ‚úÖ **Eficiencia**: CPU solo cuando hay trabajo real

---

### 6. ¬øQu√© hace exactamente el socket?

**Respuesta**:

Un **socket** es la interfaz de programaci√≥n para comunicaci√≥n de red. En nuestro caso:

#### Socket del Servidor (escucha)
```python
# Crea un socket que "escucha" conexiones entrantes
server = await asyncio.start_server(...)
```

**Internamente hace**:
1. `socket.socket(AF_INET, SOCK_STREAM)` - Crea socket TCP
2. `socket.bind(("0.0.0.0", 5001))` - Asocia a IP y puerto
3. `socket.listen(...)` - Pone en modo escucha
4. `socket.accept()` - Acepta conexiones (retorna un nuevo socket por cliente)

#### Socket del Cliente (por conexi√≥n)
```python
async def handle_client(reader, writer):
    # reader: StreamReader (wrapper de socket.recv)
    # writer: StreamWriter (wrapper de socket.send)
```

**Operaciones b√°sicas**:
- `reader.readline()` ‚Üí Lee datos del socket hasta `\n`
- `writer.write(data)` ‚Üí Escribe datos al buffer
- `writer.drain()` ‚Üí Fuerza env√≠o del buffer al socket

**A bajo nivel** (en C, simplificado):
```c
// Servidor
int server_fd = socket(AF_INET, SOCK_STREAM, 0);
bind(server_fd, addr, ...);
listen(server_fd, backlog);

while (1) {
    int client_fd = accept(server_fd, ...);  // Nuevo socket por cliente
    handle_client(client_fd);
}

// Cliente
int client_fd = socket(AF_INET, SOCK_STREAM, 0);
connect(client_fd, server_addr, ...);
send(client_fd, "Hola", 4, 0);
recv(client_fd, buffer, 1024, 0);
```

**En Python con asyncio**:
Asyncio envuelve estos sockets en objetos de alto nivel (`StreamReader/Writer`) que son **no bloqueantes** y se integran con el event loop.

---

### 7. ¬øC√≥mo garantizan que el mensaje llegue ordenado?

**Respuesta**:

**TCP garantiza el orden autom√°ticamente**. No necesitamos hacer nada especial. Aqu√≠ est√° c√≥mo:

#### Mecanismo de TCP

1. **N√∫meros de secuencia**:
   - Cada byte tiene un n√∫mero √∫nico
   - Ejemplo: "Hola mundo" se numera: H=100, o=101, l=102, ...

2. **Reordenamiento en el receptor**:
   - Si los paquetes llegan desordenados, TCP los reordena antes de entregarlos a la aplicaci√≥n

3. **ACKs (Acknowledgments)**:
   - El receptor confirma la recepci√≥n con el n√∫mero del pr√≥ximo byte esperado

**Ejemplo visual**:
```
Emisor env√≠a:
Paquete 1: [Seq=100] "Hola "
Paquete 2: [Seq=105] "mundo"

Red desordena:
Paquete 2 llega primero ‚Üê [Seq=105] "mundo"
Paquete 1 llega despu√©s ‚Üê [Seq=100] "Hola "

TCP en receptor:
Buffer: [100: "Hola ", 105: "mundo"]
Reordena por Seq: 100, 105
Entrega a app: "Hola mundo"  ‚Üê ¬°En orden!
```

#### En nuestro c√≥digo

```python
# Enviamos mensaje con \n como delimitador
writer.write("Hola\nmundo\n".encode("utf-8"))
await writer.drain()

# Receptor lee l√≠nea por l√≠nea
line1 = await reader.readline()  # "Hola\n"
line2 = await reader.readline()  # "mundo\n"
# Siempre llegan en orden por garant√≠a de TCP
```

**Nota**: Si us√°ramos UDP en vez de TCP, **no** habr√≠a garant√≠a de orden y tendr√≠amos que implementarlo nosotros con n√∫meros de secuencia adicionales.

---

### 8. ¬øQu√© ventaja aporta el gateway WebSocket?

**Respuesta**:

El gateway **desacopla el protocolo de transporte de la l√≥gica de negocio**. Ventajas clave:

#### 1. Compatibilidad con navegadores
- **Problema**: Los navegadores no pueden crear sockets TCP directos (seguridad)
- **Soluci√≥n**: WebSocket es un protocolo est√°ndar web
- El gateway traduce: WebSocket ‚Üî TCP

#### 2. Separaci√≥n de responsabilidades
```
Gateway:              Servidor:
- Maneja WebSocket    - Maneja l√≥gica de negocio
- Traduce protocolos  - Gestiona historial
- Buffer de mensajes  - Integra con LLM
```

#### 3. Extensibilidad
Agregar m√°s protocolos sin tocar el servidor:
```
HTTP REST API ‚îÄ‚îê
               ‚îú‚îÄ‚Üí [Gateway] ‚îÄ‚Üí [Servidor TCP]
WebSocket ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
               ‚îÇ
gRPC ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 4. Escalabilidad
- **Gateway horizontal**: M√∫ltiples instancias del gateway, un servidor
- **Balanceo de carga**: Distribuir clientes entre gateways

```
Cliente 1 ‚Üí Gateway A ‚îê
Cliente 2 ‚Üí Gateway A ‚îú‚îÄ‚Üí Servidor TCP
Cliente 3 ‚Üí Gateway B ‚îÇ
Cliente 4 ‚Üí Gateway B ‚îò
```

#### 5. Testing simplificado
```bash
# Probar servidor sin gateway
telnet localhost 5001
> Hola
< Usuario-1 conectado...

# Probar gateway independientemente
# Mock del servidor TCP para tests
```

**Patr√≥n de dise√±o**: **Adapter/Bridge** - Adapta una interfaz (WebSocket) a otra (TCP).

---

### 9. ¬øQu√© problema resuelve Docker?

**Respuesta**:

Docker resuelve el problema cl√°sico de **"funciona en mi m√°quina"** mediante:

#### 1. Reproducibilidad
```bash
# Desarrollador A (Windows)
docker-compose up

# Desarrollador B (Mac)
docker-compose up

# Servidor producci√≥n (Linux)
docker-compose up

# ‚Üí Mismo comportamiento en todas las m√°quinas
```

#### 2. Encapsulaci√≥n de dependencias
```dockerfile
# Todo lo necesario est√° en la imagen
FROM python:3.12-slim
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
```

**Sin Docker**:
- "¬øTienes Python 3.12?"
- "¬øInstalaste httpx?"
- "¬øTu versi√≥n de asyncio es compatible?"
- "¬øConfiguraste las variables de entorno?"

**Con Docker**: Solo necesitas Docker instalado.

#### 3. Aislamiento
- Contenedores no interfieren con el sistema host
- Cada contenedor tiene su propio:
  - Sistema de archivos
  - Espacio de procesos
  - Red virtual

#### 4. Portabilidad
```bash
# Desarrollo local
docker-compose up -d

# Subir a la nube
docker push psicoia:latest

# Deploy en AWS/GCP/Azure
docker pull psicoia:latest
docker run ...
```

#### 5. Orquestaci√≥n simplificada
```yaml
# docker-compose.yml define toda la infraestructura
services:
  app:       # Servidor TCP
  gateway:   # Gateway WebSocket
  db:        # Base de datos (futuro)
  nginx:     # Reverse proxy (futuro)
```

**Sin Docker**:
- Instalar y configurar cada servicio manualmente
- Gestionar inicio/parada de m√∫ltiples procesos
- Configurar red entre servicios

**Con Docker**:
```bash
docker-compose up -d  # Todo listo
```

#### 6. Debugging y desarrollo
```bash
# Ver logs en tiempo real
docker-compose logs -f app

# Entrar al contenedor
docker-compose exec app bash

# Reiniciar sin perder estado
docker-compose restart gateway
```

---

### 10. ¬øC√≥mo est√° organizado el c√≥digo?

**Respuesta**:

El c√≥digo sigue una **arquitectura en capas** con **separaci√≥n de concerns**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Presentaci√≥n                ‚îÇ  web/client_web.html
‚îÇ  (Interface de usuario)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ WebSocket
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Gateway Layer               ‚îÇ  gateway/ws_gateway.py
‚îÇ  (Traducci√≥n de protocolos)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ TCP
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Application Layer              ‚îÇ  app/server.py
‚îÇ  (L√≥gica de negocio)                ‚îÇ  app/client_handler.py
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ         ‚îÇ         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇServices‚îÇ ‚îÇUtils ‚îÇ ‚îÇPrompts‚îÇ
‚îÇ        ‚îÇ ‚îÇ      ‚îÇ ‚îÇ       ‚îÇ
‚îÇllm_    ‚îÇ ‚îÇrate_ ‚îÇ ‚îÇprompt ‚îÇ
‚îÇclient  ‚îÇ ‚îÇlimiter‚îÇ ‚îÇgeneral‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### M√≥dulos principales

**1. `app/server.py`** - Punto de entrada
- Inicializa el servidor TCP
- Configura asyncio
- Delega a `client_handler`

**2. `app/client_handler.py`** - Core del negocio
- Maneja ciclo de vida del cliente
- Implementa rate limiting
- Orquesta llamadas al LLM

**3. `app/services/llm_client.py`** - Integraci√≥n externa
- Cliente HTTP para Groq
- Gesti√≥n de historial
- Reintentos y timeouts

**4. `app/utils/`** - Utilidades compartidas
- `logger.py`: Logging estructurado
- `rate_limiter.py`: Control de tasa

**5. `gateway/ws_gateway.py`** - Protocolo bridge
- Servidor WebSocket
- Proxy bidireccional a TCP

**6. `app/config.py`** - Configuraci√≥n centralizada
- Variables de entorno
- Settings con Pydantic

#### Principios de dise√±o aplicados

1. **Single Responsibility**: Cada m√≥dulo tiene un prop√≥sito claro
2. **Dependency Injection**: Config se inyecta, no se importa globalmente en l√≥gica
3. **Separation of Concerns**: Gateway no sabe de LLM, LLM no sabe de sockets
4. **Open/Closed**: F√°cil agregar nuevos protocolos sin modificar el core

---

### 11. ¬øQu√© mecanismos de seguridad tiene el sistema?

**Respuesta**:

Aunque es un prototipo educativo, implementamos varias **medidas de protecci√≥n**:

#### 1. Rate Limiting (prevenci√≥n de flooding)
```python
# L√≠mite: 10 mensajes por 60 segundos por usuario
limiter = SlidingWindowLimiter(max_messages=10, window_seconds=60)
```

**Protege contra**:
- Usuarios maliciosos que env√≠an miles de mensajes
- Ataques de denegaci√≥n de servicio (DoS)
- Sobrecarga accidental

#### 2. Sem√°foro global (protecci√≥n del recurso compartido)
```python
SEM_GLOBAL = asyncio.Semaphore(MAX_IN_FLIGHT)
```

**Protege contra**:
- Sobrecarga de la API del LLM
- Costos excesivos (API cobra por request)
- Throttling del proveedor

#### 3. Timeouts (prevenci√≥n de conexiones zombies)
```python
async with httpx.AsyncClient(timeout=30) as client:
    resp = await client.post(...)
```

**Protege contra**:
- Llamadas que se cuelgan indefinidamente
- Agotamiento de recursos (sockets, memoria)

#### 4. Validaci√≥n de entrada
```python
msg = data.decode().strip()
if not msg:
    continue  # Ignora mensajes vac√≠os
```

#### 5. Manejo de excepciones
```python
try:
    # C√≥digo que puede fallar
except Exception as e:
    log.exception(f"Error: {e}")
    # Sistema sigue funcionando
```

**Mejoras de seguridad futuras**:
- [ ] Autenticaci√≥n (JWT tokens)
- [ ] Cifrado TLS/SSL
- [ ] Sanitizaci√≥n de input (prevenir injection)
- [ ] L√≠mite de longitud de mensaje
- [ ] Blacklist de IPs abusivas

---

### 12. ¬øC√≥mo escalar√≠a el sistema para producci√≥n?

**Respuesta**:

Para llevar esto a producci√≥n, implementar√≠a:

#### 1. Base de datos persistente
```python
# Reemplazar dict en RAM por PostgreSQL
async def get_history(conversation_id):
    return await db.fetch(
        "SELECT * FROM messages WHERE user_id = $1 ORDER BY timestamp",
        conversation_id
    )
```

#### 2. Balanceo de carga
```
              ‚îå‚îÄ‚Üí Servidor 1
Load Balancer ‚îú‚îÄ‚Üí Servidor 2
              ‚îî‚îÄ‚Üí Servidor 3
```

#### 3. Cache distribuido
```python
# Redis para historial de sesi√≥n
import aioredis
redis = await aioredis.create_redis_pool("redis://localhost")
history = await redis.get(f"session:{user_id}")
```

#### 4. Message Queue para desacoplamiento
```
Cliente ‚Üí Gateway ‚Üí RabbitMQ ‚Üí Workers ‚Üí LLM
```

**Ventajas**:
- Procesamiento as√≠ncrono
- Reintentos autom√°ticos
- Escalado independiente de workers

#### 5. Observabilidad profesional
```python
# Prometheus para m√©tricas
from prometheus_client import Counter, Histogram

requests_total = Counter("requests_total", "Total requests")
latency = Histogram("llm_latency_seconds", "LLM latency")
```

#### 6. Seguridad robusta
- **TLS/SSL**: Cifrado en tr√°nsito
- **Autenticaci√≥n**: JWT o OAuth2
- **Rate limiting distribuido**: Redis + sliding window
- **WAF**: Web Application Firewall

#### 7. Kubernetes para orquestaci√≥n
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: psicoia-server
spec:
  replicas: 5  # 5 instancias del servidor
  template:
    spec:
      containers:
      - name: app
        image: psicoia:latest
```

**Auto-scaling**:
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
spec:
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

---

### 13. ¬øQu√© pasa si el LLM falla o tarda mucho?

**Respuesta**:

Tenemos **m√∫ltiples capas de resiliencia**:

#### 1. Timeout configurable
```python
async with httpx.AsyncClient(timeout=30) as client:
    resp = await client.post(...)
```

Si el LLM no responde en 30 segundos ‚Üí TimeoutError

#### 2. Reintentos con backoff exponencial
```python
for attempt in range(1, MAX_RETRIES + 1):
    try:
        resp = await client.post(...)
        if resp.status_code == 429:  # Too Many Requests
            wait = BACKOFF_INITIAL * (2 ** (attempt - 1))
            await asyncio.sleep(wait)
            continue
        break
    except httpx.RequestError:
        # Reintenta
        continue
```

**Tiempos de espera**:
- Intento 1: 0s
- Intento 2: 2s (backoff)
- Intento 3: 4s (backoff √ó 2)
- Intento 4: 8s (backoff √ó 4)

#### 3. Respuesta de fallback
```python
except Exception as e:
    log.error(f"Groq error: {e}")
    return "Ocurri√≥ un error. Por favor, intent√° de nuevo."
```

El usuario recibe un mensaje claro en vez de timeout silencioso.

#### 4. Sem√°foro protege al sistema
```python
async with SEM_GLOBAL:  # Solo N pueden llamar al LLM simult√°neamente
    llm_reply = await llm_generate(...)
```

Si el LLM est√° lento, otros usuarios esperan en la cola en vez de abrumar la API.

#### 5. Logs detallados para debugging
```python
log.warning(f"[{trace_id}] LLM retry {attempt}/{MAX_RETRIES} in {wait:.2f}s")
log.error(f"[{trace_id}] Groq error: {e}")
```

**Mejora futura**: Circuit breaker pattern
```python
# Si 5 requests fallan seguidos, deja de intentar por 60s
# Evita saturar el LLM si est√° ca√≠do
```

---

### 14. ¬øPor qu√© usan `readline()` en vez de `read()`?

**Respuesta**:

`readline()` es m√°s apropiado para protocolos basados en texto:

#### Ventajas de readline()
```python
data = await reader.readline()  # Lee hasta \n
```

1. **Delimitaci√≥n natural**: Cada l√≠nea es un mensaje completo
2. **Buffer autom√°tico**: Si llega medio mensaje, espera el resto
3. **Simplicidad**: No necesitamos implementar nuestro propio protocolo de framing

#### Con read() necesitar√≠amos:
```python
# ‚ùå M√°s complejo
data = await reader.read(1024)  # ¬øCu√°ntos bytes leer?
# - ¬øQu√© pasa si el mensaje es m√°s largo?
# - ¬øQu√© pasa si llegan 2 mensajes juntos?
# - Necesitamos buffer y l√≥gica de parsing
```

#### Protocolo de mensajes
```
Cliente env√≠a: "Hola\n"
              "¬øC√≥mo est√°s?\n"

readline() retorna:
Primera llamada: "Hola\n"
Segunda llamada: "¬øC√≥mo est√°s?\n"

# Cada mensaje est√° perfectamente delimitado
```

**Desventaja**: No apto para datos binarios o mensajes que contengan `\n` en el medio. Para eso usar√≠amos un protocolo con longitud prefijada.

---

### 15. ¬øQu√© es `writer.drain()` y por qu√© es necesario?

**Respuesta**:

`drain()` es cr√≠tico para el **control de flujo** y garantiza que los datos se env√≠en.

#### Sin drain()
```python
writer.write("Hola".encode())  # Solo escribe al buffer
# Los datos NO se env√≠an inmediatamente al socket
```

#### Con drain()
```python
writer.write("Hola".encode())  # Escribe al buffer
await writer.drain()            # Espera a que se env√≠e todo
```

**¬øQu√© hace drain() internamente?**
1. Fuerza el flush del buffer al socket
2. Si el buffer est√° lleno, **espera** (yield) hasta que haya espacio
3. Implementa **backpressure**: si el cliente lee lento, el servidor espera

#### Escenario sin drain()
```python
# Servidor r√°pido, cliente lento
for i in range(10000):
    writer.write(f"Mensaje {i}\n".encode())
    # Sin drain(), el buffer crece indefinidamente
    # ‚Üí Out of Memory
```

#### Con drain() (correcto)
```python
for i in range(10000):
    writer.write(f"Mensaje {i}\n".encode())
    await writer.drain()  # Espera si el buffer est√° lleno
    # ‚Üí Control de flujo autom√°tico
```

**Analog√≠a**: Es como un grifo con sensor. Si el recipiente est√° lleno, el grifo espera antes de seguir vertiendo agua.

---

### 16. ¬øQu√© pasa si un cliente se desconecta abruptamente?

**Respuesta**:

Tenemos manejo robusto de desconexiones:

#### Detecci√≥n de desconexi√≥n
```python
while True:
    data = await reader.readline()
    if not data:  # Socket cerrado
        break
```

Cuando el cliente cierra la conexi√≥n:
1. `reader.readline()` retorna `b''` (bytes vac√≠o)
2. El loop detecta esto y sale con `break`
3. El bloque `finally` limpia recursos

#### Cleanup autom√°tico
```python
try:
    # Manejo del cliente
    pass
except Exception as e:
    log.exception(f"[{user}] Error: {e}")
finally:
    writer.close()              # Cierra el socket
    await writer.wait_closed()  # Espera confirmaci√≥n
    log.info(f"[{user}] Conexi√≥n cerrada")
```

#### Escenarios cubiertos

1. **Cierre limpio** (cliente env√≠a FIN):
   - TCP notifica EOF
   - `readline()` retorna vac√≠o
   - Limpieza ordenada

2. **Cierre abrupto** (cable desconectado, proceso matado):
   - TCP espera timeout
   - Luego notifica error
   - Exception capturada en `except`

3. **Network partition**:
   - Keep-alive de TCP detecta conexi√≥n muerta
   - Socket genera error despu√©s de varios reintentos
   - Limpieza en `finally`

#### Historial del usuario
```python
# Al desconectar, el historial en RAM se MANTIENE
# Si se reconecta, obtiene nuevo ID pero puede recuperar sesi√≥n
# (aunque en la implementaci√≥n actual, cada conexi√≥n = nueva sesi√≥n)
```

**Mejora futura**: Sesiones persistentes con tokens para reconectar y mantener contexto.

---

### 17. ¬øC√≥mo funciona el sem√°foro exactamente?

**Respuesta**:

El sem√°foro es un **contador at√≥mico** que limita el acceso concurrente a un recurso.

#### Concepto
```python
SEM_GLOBAL = asyncio.Semaphore(2)  # Contador inicial: 2
```

**Estado interno**:
- `_value`: Contador actual (2, 1, 0, ...)
- `_waiters`: Cola de coroutines esperando

#### Flujo de ejecuci√≥n

```python
# Estado inicial: value=2

# Cliente A llega
async with SEM_GLOBAL:  # value=2-1=1, ENTRA
    await llm_generate()  # Llama al LLM

# Cliente B llega (mientras A espera)
async with SEM_GLOBAL:  # value=1-1=0, ENTRA
    await llm_generate()  # Llama al LLM

# Cliente C llega (mientras A y B esperan)
async with SEM_GLOBAL:  # value=0, BLOQUEA
    # C se agrega a _waiters y cede control
    await llm_generate()

# A termina
# ‚Üí Sale del contexto
# ‚Üí value=0+1=1
# ‚Üí Despierta a C de _waiters
# ‚Üí C entra y value=1-1=0

# B termina
# ‚Üí value=0+1=1
# ‚Üí Si hay m√°s en _waiters, despierta al siguiente
```

#### Visualizaci√≥n
```
Tiempo ‚Üí
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Cliente A: ‚îÄ‚îÄ‚îÄ‚îÄ[LLM]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Cliente B:   ‚îÄ‚îÄ‚îÄ‚îÄ[LLM]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Cliente C:     ‚è∏Ô∏è[ESPERA]‚è∏Ô∏è‚îÄ‚îÄ‚îÄ[LLM]‚îÄ‚îÄ‚îÄ‚îÄ
Cliente D:         ‚è∏Ô∏è[ESPERA]‚è∏Ô∏è‚îÄ‚îÄ‚îÄ‚îÄ[LLM]‚îÄ‚îÄ‚îÄ
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Sem√°foro:  2‚Üí1‚Üí0    ‚Üí1‚Üí0   ‚Üí1‚Üí0
```

#### Sin sem√°foro
```python
# ‚ùå Todos llamar√≠an al LLM simult√°neamente
# ‚Üí Sobrecarga del servidor
# ‚Üí Rate limiting de Groq
# ‚Üí Costos excesivos
# ‚Üí Timeouts
```

#### Con sem√°foro
```python
# ‚úÖ M√°ximo N llamadas simult√°neas
# ‚Üí Uso controlado de recursos
# ‚Üí Cola de espera justa (FIFO)
# ‚Üí Protecci√≥n contra sobrecarga
```

---

### 18. ¬øD√≥nde exactamente se usa paralelismo vs concurrencia?

**Respuesta**:

En nuestro proyecto usamos **concurrencia**, NO paralelismo real:

#### Definiciones

**Concurrencia** (lo que usamos):
- **Un solo hilo/n√∫cleo** ejecuta m√∫ltiples tareas
- Las tareas se intercalan (time-slicing cooperativo)
- Cuando una espera I/O, otra ejecuta

**Paralelismo** (NO usado):
- **M√∫ltiples hilos/n√∫cleos** ejecutan tareas simult√°neamente
- Ejecuci√≥n realmente paralela en CPUs diferentes

#### En nuestro c√≥digo

```python
# CONCURRENCIA (asyncio)
# Un solo thread del event loop
async def handle_client(reader, writer):
    while True:
        data = await reader.readline()  # Cede control aqu√≠
        response = await llm_generate(data)  # Y aqu√≠
        await writer.drain()  # Y aqu√≠

# M√∫ltiples clientes ‚Üí m√∫ltiples coroutines
# Pero todas en UN SOLO THREAD
```

**Si us√°ramos paralelismo**:
```python
# PARALELISMO (multiprocessing)
from multiprocessing import Process

def handle_client(socket):
    # Cada cliente en su propio proceso
    # Realmente paralelo en m√∫ltiples CPUs
    pass

for connection in connections:
    p = Process(target=handle_client, args=(connection,))
    p.start()
```

#### ¬øPor qu√© NO necesitamos paralelismo?

Nuestro trabajo es **I/O-bound**:
```
Tiempo de procesamiento:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Esperando red: 95%          ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
‚îÇ Procesando CPU: 5%          ‚îÇ‚ñà
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

# CPU casi siempre est√° idle esperando I/O
# No tiene sentido usar m√∫ltiples CPUs
```

Si fuera **CPU-bound** (ej: procesamiento de video, c√°lculos complejos):
```
Tiempo de procesamiento:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Esperando I/O: 5%           ‚îÇ‚ñà
‚îÇ Procesando CPU: 95%         ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

# Aqu√≠ S√ç necesitar√≠amos paralelismo
# Para usar m√∫ltiples n√∫cleos
```

#### Tabla resumen

| Aspecto | Concurrencia (nuestro) | Paralelismo (no usado) |
|---------|------------------------|------------------------|
| **Threads/Procesos** | 1 thread | M√∫ltiples threads/procesos |
| **Ejecuci√≥n** | Intercalada | Simult√°nea real |
| **Ideal para** | I/O-bound | CPU-bound |
| **Implementaci√≥n** | `asyncio` | `multiprocessing`, `threading` |
| **Complejidad** | Baja | Alta (race conditions) |
| **Overhead** | Muy bajo | Alto (context switches) |

---

### 19. ¬øQu√© es el "event loop" exactamente? ¬øC√≥mo funciona internamente?

**Respuesta**:

El **event loop** es el scheduler que coordina todas las coroutines. Es un bucle infinito que:

#### Pseudoc√≥digo del event loop
```python
# Simplificaci√≥n conceptual
class EventLoop:
    def __init__(self):
        self.ready_queue = []      # Tareas listas para ejecutar
        self.io_waiting = {}       # Tareas esperando I/O
        self.selector = Selector()  # Monitorea file descriptors
    
    def run_forever(self):
        while True:
            # 1. Ejecutar tareas listas
            while self.ready_queue:
                task = self.ready_queue.pop(0)
                task.run_until_next_await()  # Ejecuta hasta el pr√≥ximo await
            
            # 2. Revisar qu√© I/O est√° listo
            ready_fds = self.selector.select(timeout=0.1)
            
            # 3. Mover tareas con I/O listo a ready_queue
            for fd in ready_fds:
                task = self.io_waiting.pop(fd)
                self.ready_queue.append(task)
            
            # 4. Si no hay nada que hacer, esperar eventos I/O
            if not self.ready_queue:
                ready_fds = self.selector.select(timeout=None)  # Bloquea
```

#### Ejemplo concreto

```python
# Tienes 3 clientes conectados
async def client1():
    data = await reader.readline()  # ‚Üê Espera I/O (socket #5)
    process(data)

async def client2():
    response = await llm_generate()  # ‚Üê Espera I/O (HTTP socket #7)
    return response

async def client3():
    result = compute()  # ‚Üê CPU, no espera
    return result
```

**Estado del event loop**:
```
Iteraci√≥n 1:
  ready_queue: [client3]
  io_waiting: {5: client1, 7: client2}
  
  ‚Üí Ejecuta client3 (computa)
  ‚Üí Termina
  ‚Üí ready_queue vac√≠a
  
  ‚Üí select() revisa sockets: ninguno listo
  ‚Üí Espera...

Iteraci√≥n 2:
  select() detecta: socket #5 tiene datos
  
  ‚Üí Mueve client1 a ready_queue
  ‚Üí ready_queue: [client1]
  ‚Üí Ejecuta client1 hasta process(data)
  ‚Üí Si process tiene await, vuelve a io_waiting
  ‚Üí Si no, termina

Iteraci√≥n 3:
  select() detecta: socket #7 tiene respuesta HTTP
  
  ‚Üí Mueve client2 a ready_queue
  ‚Üí Ejecuta hasta return response
  ‚Üí Termina
```

#### Integraci√≥n con el OS

```python
# Event loop usa select/epoll/kqueue del OS
import select

# Registra sockets para monitorear
selector.register(socket_fd, EVENT_READ, callback)

# OS notifica cuando hay datos
ready = selector.select(timeout=1.0)
# ‚Üí Retorna lista de file descriptors listos
```

**Ventaja**: El event loop NO hace polling activo. Usa llamadas del sistema operativo que bloquean eficientemente hasta que hay eventos.

---

### 20. ¬øCu√°les son las operaciones admitidas y limitaciones del sistema?

**Respuesta**:

#### ‚úÖ Operaciones Admitidas (10 caracter√≠sticas clave)

**1. Conversaciones simult√°neas con aislamiento de estado**
```python
# Cada usuario tiene su propio:
user = f"Usuario-{next(USER_SEQ)}"         # ID √∫nico
limiter = SlidingWindowLimiter(...)         # Rate limiter exclusivo
_histories[conversation_id] = [...]         # Historial aislado
```

**2. Limitaci√≥n de velocidad configurable**
```python
# En .env:
RATE_MAX_MESSAGES=10        # Mensajes permitidos
RATE_WINDOW_SECONDS=60      # En esta ventana de tiempo

# Si el usuario excede el l√≠mite:
if not limiter.allow():
    return "Demasiados mensajes. Esper√° unos segundos."
```

**3. Control de contrapresi√≥n global**
```python
SEM_GLOBAL = asyncio.Semaphore(settings.MAX_IN_FLIGHT)

# Solo N llamadas simult√°neas al LLM
async with SEM_GLOBAL:
    llm_reply = await llm_generate(...)
```

**4. Reintentos con backoff exponencial**
```python
for attempt in range(1, MAX_RETRIES + 1):
    try:
        resp = await client.post(...)
        if resp.status_code == 429:  # Too Many Requests
            wait = BACKOFF_INITIAL * (2 ** (attempt - 1))
            await asyncio.sleep(wait)  # 2s, 4s, 8s, 16s...
            continue
    except httpx.RequestError:
        continue
```

**5. Ajuste de ventana de tokens**
```python
def build_messages(system_prompt, history, user_text):
    budget = LLM_INPUT_TOKEN_BUDGET  # Ej: 2000 tokens
    
    # Incluir solo mensajes que quepan en el budget
    picked = []
    for msg in reversed(history):
        tokens = estimate_tokens(msg)
        if total + tokens > budget:
            break  # No cabe m√°s
        picked.append(msg)
    
    return [system_msg] + picked + [user_msg]
```

**6. Degradaci√≥n controlada sin API**
```python
if not settings.GROQ_API_KEY:
    # Modo offline con respuesta por defecto
    return "Estoy aqu√≠ para acompa√±arte. Prob√° respirar suave..."
```

**7. Conectividad dual (WebSocket + TCP)**
```python
# Opci√≥n A: Navegador ‚Üí WebSocket ‚Üí Gateway ‚Üí TCP ‚Üí Servidor
ws://localhost:8765

# Opci√≥n B: Cliente nativo ‚Üí TCP directo ‚Üí Servidor
telnet localhost 5001
```

**8. Trace ID para depuraci√≥n**
```python
trace_id = f"{user}:m{msg_counter}"

log.info(f"[{trace_id}] ‚Üí LLM start")
# Header HTTP:
headers["X-Request-ID"] = trace_id
log.info(f"[{trace_id}] ‚Üê LLM ok ({dt_ms:.0f} ms)")

# Permite correlacionar toda la request end-to-end
```

**9. Observabilidad completa**
```python
# Logs estructurados con contexto
log.info(f"[{user}] Conexi√≥n desde {peer}")
log.info(f"[{trace_id}] POST {url} model={model}")
log.warning(f"[{user}] Rate limit excedido")
log.error(f"[{trace_id}] Groq error: {e}")

# M√©tricas disponibles:
# - Latencia del LLM
# - Tasa de rate limiting
# - Conexiones activas
# - Errores por tipo
```

**10. Containerizaci√≥n completa**
```yaml
# docker-compose.yml
services:
  app:      # Servidor TCP
  gateway:  # Gateway WebSocket

# Un solo comando:
docker-compose up -d
```

---

#### ‚ö†Ô∏è Limitaciones Conocidas (7 limitaciones por dise√±o)

**1. Historial en RAM (no persistente)**
```python
_histories: dict[str, list[dict]] = defaultdict(list)
# ‚ùå Se pierde al reiniciar
# ‚úÖ Migraci√≥n futura: PostgreSQL/MongoDB
```

**Por qu√© es as√≠**: Prototipo educativo. M√°s simple de entender y suficiente para demo.

**2. Sin autenticaci√≥n**
```python
user = f"Usuario-{next(USER_SEQ)}"
# ‚ùå Cualquiera puede conectarse
# ‚ùå No hay verificaci√≥n de identidad
# ‚úÖ Migraci√≥n futura: JWT tokens + OAuth2
```

**3. Sin persistencia de conversaciones**
```python
# ‚ùå No se guarda en DB
# ‚ùå No se puede recuperar sesi√≥n anterior
# ‚úÖ Migraci√≥n futura: 
#    CREATE TABLE messages (user_id, content, timestamp)
```

**4. Instancia √∫nica (no escalable horizontalmente)**
```python
# ‚ùå Un solo servidor
# ‚ùå Si se cae, todo se cae
# ‚ùå No se puede distribuir carga

# ‚úÖ Migraci√≥n futura:
#    - Redis para estado compartido
#    - Load Balancer con m√∫ltiples instancias
#    - Session affinity o state replication
```

**5. Sin cifrado**
```python
# ‚ùå HTTP plano (no HTTPS)
# ‚ùå WebSocket plano (no WSS)
# ‚ùå Datos visibles en la red

# ‚úÖ Migraci√≥n futura:
#    - Certificados SSL/TLS
#    - nginx como reverse proxy con HTTPS
#    - wss:// en vez de ws://
```

**6. Identificaci√≥n temporal**
```python
user = f"Usuario-{next(USER_SEQ)}"
# ‚ùå Solo v√°lido mientras dura la conexi√≥n
# ‚ùå Usuario-1 hoy ‚â† Usuario-1 ma√±ana

# ‚úÖ Migraci√≥n futura:
#    - UUID persistente
#    - user_id en DB
```

**7. Sin recuperaci√≥n de sesi√≥n**
```python
# ‚ùå Si el cliente se desconecta:
#    - Pierde su Usuario-N
#    - Pierde su historial
#    - No puede reconectar a la misma sesi√≥n

# ‚úÖ Migraci√≥n futura:
#    - Session tokens
#    - async def reconnect(token)
#    - Recuperar historial de DB
```

---

#### Tabla Comparativa

| Aspecto | Estado Actual | Producci√≥n Requerir√≠a |
|---------|---------------|----------------------|
| **Estado** | RAM vol√°til | Redis/DB persistente |
| **Autenticaci√≥n** | Ninguna | JWT + OAuth2 |
| **Cifrado** | HTTP plano | TLS/SSL |
| **Escalado** | Vertical only | Horizontal + LB |
| **Observabilidad** | Logs b√°sicos | Prometheus + Grafana |
| **Alta disponibilidad** | No | Multi-regi√≥n + failover |
| **Backup** | No | Snapshots + replicaci√≥n |

---

#### Justificaci√≥n de las Limitaciones

**¬øPor qu√© no implementamos todo desde el inicio?**

1. **Objetivo educativo**: Prioridad es entender conceptos (sockets, asyncio, concurrencia)
2. **Simplicidad**: Menos c√≥digo = m√°s f√°cil de entender y defender
3. **Tiempo de desarrollo**: Proyecto funcional en tiempo razonable
4. **Suficiencia**: Cumple los requisitos acad√©micos
5. **Extensibilidad**: Arquitectura preparada para agregar features

**El profesor valorar√°**:
- ‚úÖ Que entiendas las limitaciones
- ‚úÖ Que puedas justificarlas
- ‚úÖ Que sepas c√≥mo resolverlas

**No est√° mal tener limitaciones en un prototipo**. Est√° mal **no conocerlas** o **no saber c√≥mo superarlas**.

---

### 21. ¬øC√≥mo prueban que el sistema realmente maneja m√∫ltiples usuarios?

**Respuesta**:

Podemos demostrarlo con **pruebas de carga** y **logs estructurados**:

#### 1. Test manual con m√∫ltiples navegadores
```bash
# Terminal 1: Levantar el sistema
docker-compose up

# Navegador 1: http://localhost:8000/client_web.html
# Navegador 2: http://localhost:8000/client_web.html (otra pesta√±a)
# Navegador 3: http://localhost:8000/client_web.html (otra pesta√±a)

# Los tres pueden enviar mensajes simult√°neamente
# Los logs mostrar√°n:
[Usuario-1] Conexi√≥n desde ('127.0.0.1', 50234)
[Usuario-2] Conexi√≥n desde ('127.0.0.1', 50235)
[Usuario-3] Conexi√≥n desde ('127.0.0.1', 50236)
```

#### 2. Script de prueba de carga
```python
# load_test.py
import asyncio
import websockets

async def client(client_id, num_messages):
    uri = "ws://localhost:8765"
    async with websockets.connect(uri) as ws:
        # Recibir saludo
        greeting = await ws.recv()
        print(f"Cliente-{client_id}: {greeting}")
        
        # Enviar mensajes
        for i in range(num_messages):
            await ws.send(f"Cliente-{client_id}: Mensaje {i}")
            response = await ws.recv()
            print(f"Cliente-{client_id} recibi√≥: {response[:50]}...")
            await asyncio.sleep(0.5)

async def main():
    # Crear 100 clientes simult√°neos
    tasks = [client(i, 5) for i in range(100)]
    await asyncio.gather(*tasks)

asyncio.run(main())
```

**Ejecutar**:
```bash
python load_test.py

# Output esperado:
Cliente-0: Usuario-1 conectado...
Cliente-1: Usuario-2 conectado...
Cliente-2: Usuario-3 conectado...
...
Cliente-99: Usuario-100 conectado...

# Los 100 pueden enviar/recibir simult√°neamente
```

#### 3. Verificaci√≥n en logs
```bash
docker-compose logs -f app | grep "Conexi√≥n desde"

# Ver√°s m√∫ltiples conexiones activas:
[Usuario-1] Conexi√≥n desde ('172.18.0.1', 50234)
[Usuario-1:m1] ‚Üí LLM start (len=25)
[Usuario-2] Conexi√≥n desde ('172.18.0.1', 50235)
[Usuario-2:m1] ‚Üí LLM start (len=30)
[Usuario-1:m1] ‚Üê LLM ok (150 chars) 892 ms
[Usuario-3] Conexi√≥n desde ('172.18.0.1', 50236)
[Usuario-2:m1] ‚Üê LLM ok (180 chars) 1105 ms
[Usuario-3:m1] ‚Üí LLM start (len=22)

# Las operaciones se intercalan = concurrencia real
```

#### 4. M√©tricas del sistema
```bash
# Ver cu√°ntas conexiones TCP est√°n activas
docker-compose exec app netstat -ant | grep 5001

# Output:
tcp  0  0  172.18.0.2:5001  172.18.0.1:50234  ESTABLISHED
tcp  0  0  172.18.0.2:5001  172.18.0.1:50235  ESTABLISHED
tcp  0  0  172.18.0.2:5001  172.18.0.1:50236  ESTABLISHED
# ‚Üí 3 conexiones activas simult√°neas
```

#### 5. Test con telnet
```bash
# Terminal 1
telnet localhost 5001
> Hola, soy el usuario 1

# Terminal 2 (mientras el 1 espera respuesta)
telnet localhost 5001
> Hola, soy el usuario 2

# Terminal 3
telnet localhost 5001
> Hola, soy el usuario 3

# Los tres obtienen respuestas sin bloquearse entre s√≠
```

**Prueba definitiva**: Simular un usuario "lento":
```python
# Cliente que tarda mucho en leer
async def slow_client():
    async with websockets.connect(uri) as ws:
        await ws.send("Mensaje 1")
        await asyncio.sleep(60)  # Espera 60 segundos sin leer
        response = await ws.recv()

# Mientras tanto, otros 100 clientes funcionan normalmente
# ‚Üí Demuestra que un cliente lento NO bloquea a los dem√°s
```

---

## Conclusi√≥n

Este proyecto demuestra conocimiento profundo de:

‚úÖ **Networking**: Sockets, TCP, WebSocket  
‚úÖ **Concurrencia**: AsyncIO, event loop, coroutines  
‚úÖ **Arquitectura**: Separation of concerns, layered architecture  
‚úÖ **Observabilidad**: Logging, tracing, m√©tricas  
‚úÖ **DevOps**: Docker, containerizaci√≥n, orquestaci√≥n  
‚úÖ **Resiliencia**: Timeouts, reintentos, rate limiting  
‚úÖ **Integraci√≥n**: APIs REST, HTTP async, manejo de errores  

El sistema es **funcional**, **escalable** y **mantenible**, con decisiones t√©cnicas bien justificadas y c√≥digo limpio y documentado.

---

---

## üìö RESUMEN EJECUTIVO PARA LA DEFENSA

### ‚úÖ Lo que el Sistema PUEDE hacer

| Caracter√≠stica | Implementaci√≥n | D√≥nde est√° |
|----------------|----------------|------------|
| **M√∫ltiples usuarios simult√°neos** | Coroutine por cliente | `client_handler.py` |
| **Rate limiting por usuario** | Ventana deslizante | `rate_limiter.py` |
| **Control de concurrencia LLM** | Sem√°foro global | `SEM_GLOBAL` en `client_handler.py` |
| **Reintentos autom√°ticos** | Backoff exponencial | `llm_client.py` |
| **Ventana de tokens adaptativa** | Budget + sliding window | `build_messages()` en `llm_client.py` |
| **Fallback sin API** | Respuesta por defecto | `llm_generate()` cuando no hay API key |
| **WebSocket + TCP** | Gateway + Servidor | `ws_gateway.py` + `server.py` |
| **Trazabilidad completa** | trace_id: Usuario-N:mM | Logs en todos los m√≥dulos |
| **Logs estructurados** | Niveles + formato | `logger.py` |
| **Containerizaci√≥n** | Docker Compose | `docker-compose.yml` |

### ‚ö†Ô∏è Lo que el Sistema NO puede hacer (por dise√±o)

| Limitaci√≥n | Motivo | Soluci√≥n futura |
|------------|--------|-----------------|
| **Persistencia de historial** | RAM vol√°til | PostgreSQL/MongoDB |
| **Autenticaci√≥n de usuarios** | Sin sistema de cuentas | JWT + OAuth2 |
| **Cifrado TLS/SSL** | HTTP/WS plano | Certificados + nginx |
| **Escalado horizontal** | Estado en RAM local | Redis + Load Balancer |
| **Recuperaci√≥n de sesi√≥n** | ID temporal | Tokens persistentes |
| **Multi-regi√≥n** | Instancia √∫nica | Deploy distribuido |

### Conceptos Clave que DEBES dominar

#### üîå SOCKET
**Qu√© es**: Punto final de comunicaci√≥n en red (IP + puerto)  
**D√≥nde est√°**: `app/server.py` (l√≠nea con `asyncio.start_server`)  
**C√≥mo funciona**: Se crea con `socket()`, se asocia con `bind()`, escucha con `listen()`, acepta con `accept()`

#### üåê TCP
**Qu√© es**: Protocolo confiable que garantiza orden y entrega  
**Por qu√© orden**: N√∫meros de secuencia + reordenamiento en receptor  
**Ventaja**: No tenemos que preocuparnos por paquetes perdidos o desordenados

#### ‚ö° ASYNCIO
**Qu√© es**: Concurrencia cooperativa sin hilos  
**Por qu√© usarlo**: I/O-bound (esperamos red), miles de coroutines con 1 hilo  
**Alternativa**: Hilos = 8MB cada uno, m√°ximo 1000 clientes

#### üîÑ CONCURRENCIA vs PARALELISMO
**Concurrencia** (nuestro caso): 1 CPU, m√∫ltiples tareas intercaladas  
**Paralelismo** (NO usamos): M√∫ltiples CPUs, ejecuci√≥n simult√°nea real  
**Por qu√© concurrencia**: Nuestro c√≥digo espera 95% del tiempo (I/O-bound)

#### üö¶ NIVEL DE CONCURRENCIA
1. **Conexiones**: Ilimitadas (limitado por SO)
2. **LLM simult√°neos**: Limitado por sem√°foro (`MAX_IN_FLIGHT`)
3. **Mensajes/usuario**: Limitado por rate limiter (10 msg/60s)

#### üë• MULTIUSUARIO
**C√≥mo identificamos**: `Usuario-N` (contador secuencial)  
**C√≥mo aislamos**: Cada cliente tiene su socket, rate limiter e historial  
**Clave √∫nica**: `conversation_id` para el historial en RAM

#### üìä OBSERVABILIDAD
**Logs**: Niveles (debug, info, warning, error)  
**Trazabilidad**: `trace_id` = `Usuario-N:mM` vincula todas las operaciones  
**M√©tricas**: Latencia LLM, rate limiting, errores

#### üê≥ DOCKER
**Por qu√©**: Reproducibilidad + aislamiento + portabilidad  
**Arquitectura**: `app` (servidor TCP) + `gateway` (WebSocket)  
**Red interna**: Los contenedores se comunican por nombre de servicio

### Flujo Completo del Sistema

```
1. Usuario escribe en navegador
   ‚Üì
2. JavaScript env√≠a por WebSocket al gateway (puerto 8765)
   ‚Üì
3. Gateway abre conexi√≥n TCP al servidor (puerto 5001)
   ‚Üì
4. Servidor crea coroutine handle_client()
   ‚Üì
5. Se asigna Usuario-N
   ‚Üì
6. Se valida rate limit
   ‚Üì
7. Sem√°foro limita concurrencia al LLM
   ‚Üì
8. llm_client hace POST HTTP a Groq
   ‚Üì
9. Se guarda en historial (RAM)
   ‚Üì
10. Respuesta vuelve por el mismo camino
   ‚Üì
11. Usuario ve mensaje en navegador
```

### D√≥nde est√° cada cosa en el c√≥digo

| Concepto | Archivo | L√≠nea aproximada |
|----------|---------|------------------|
| Socket de escucha | `app/server.py` | `asyncio.start_server(...)` |
| Manejo de cliente | `app/client_handler.py` | `async def handle_client(...)` |
| Rate limiting | `app/utils/rate_limiter.py` | `class SlidingWindowLimiter` |
| Sem√°foro | `app/client_handler.py` | `SEM_GLOBAL = asyncio.Semaphore(...)` |
| Historial | `app/services/llm_client.py` | `_histories: dict[str, list[dict]]` |
| Gateway WS‚ÜîTCP | `gateway/ws_gateway.py` | `async def bridge_ws_to_tcp(...)` |
| Event loop | `app/server.py` | `asyncio.run(main())` |
| Logs | `app/utils/logger.py` | `get_logger(...)` |

### Preguntas que el profesor DEFINITIVAMENTE har√°

1. ‚úÖ **"Explicame c√≥mo funciona el servidor"**  
   ‚Üí Ver secci√≥n "¬øC√≥mo funciona el servidor?" (pregunta 1)

2. ‚úÖ **"¬øC√≥mo distingu√≠s qui√©n es cada usuario?"**  
   ‚Üí `Usuario-N` secuencial + socket √∫nico + historial por `conversation_id`

3. ‚úÖ **"¬øQu√© pasa si dos usuarios env√≠an mensaje al mismo tiempo?"**  
   ‚Üí Est√°n aislados, cada uno tiene su coroutine independiente

4. ‚úÖ **"Explicame qu√© es un socket"**  
   ‚Üí Punto final de comunicaci√≥n, IP + puerto, analog√≠a: n√∫mero de tel√©fono

5. ‚úÖ **"¬øPor qu√© usaron asyncio en vez de hilos?"**  
   ‚Üí I/O-bound, miles de coroutines vs 1000 hilos, menos memoria

6. ‚úÖ **"¬øC√≥mo llegan los mensajes en orden?"**  
   ‚Üí TCP garantiza con n√∫meros de secuencia + reordenamiento

7. ‚úÖ **"Mostr√° d√≥nde est√° el socket en el c√≥digo"**  
   ‚Üí `app/server.py`: `asyncio.start_server()`

8. ‚úÖ **"Explicame Docker"**  
   ‚Üí Reproducibilidad, aislamiento, `docker-compose up` y funciona

9. ‚úÖ **"¬øQu√© es concurrencia?"**  
   ‚Üí M√∫ltiples tareas progresando intercal√°ndose (1 CPU)

10. ‚úÖ **"Simulemos que se cae la red"**  
    ‚Üí `docker-compose stop app` ‚Üí gateway notifica error al cliente

### Respuestas cortas para preguntas r√°pidas

**¬øQu√© es TCP?** ‚Üí Protocolo confiable con orden garantizado  
**¬øQu√© es un socket?** ‚Üí Punto final de comunicaci√≥n (IP + puerto)  
**¬øQu√© es asyncio?** ‚Üí Concurrencia cooperativa sin hilos  
**¬øQu√© es un hilo?** ‚Üí Unidad de ejecuci√≥n, ~8MB cada uno  
**¬øQu√© es concurrencia?** ‚Üí M√∫ltiples tareas intercal√°ndose  
**¬øQu√© es paralelismo?** ‚Üí Ejecuci√≥n simult√°nea real (m√∫ltiples CPUs)  
**¬øNivel de concurrencia?** ‚Üí Conexiones ilimitadas, LLM limitado por sem√°foro  
**¬øMultiusuario c√≥mo?** ‚Üí Una coroutine por cliente, aislamiento total  
**¬øObservabilidad?** ‚Üí Logs estructurados + trace_id + m√©tricas  
**¬øPor qu√© Docker?** ‚Üí Reproducibilidad y aislamiento  
**¬øPor qu√© gateway?** ‚Üí Navegadores no hablan TCP, solo WebSocket  
**¬øHistorial d√≥nde?** ‚Üí RAM (dict con conversation_id como clave)  

### Tips para la Defensa

1. **Siempre menciona el trace_id**: Demuestra que entend√©s observabilidad
2. **Relaciona conceptos**: "El socket es no bloqueante gracias a asyncio"
3. **Usa analog√≠as**: Socket = tel√©fono, Event loop = director de orquesta
4. **Mostrar logs reales**: Abr√≠ `docker-compose logs -f` durante la demo
5. **Demos en vivo**: Abr√≠ m√∫ltiples pesta√±as del navegador para mostrar multiusuario
6. **C√≥digo espec√≠fico**: No digas "en el c√≥digo", dec√≠ "en app/server.py l√≠nea 15"
7. **Justifica decisiones**: No digas "usamos asyncio", dec√≠ "usamos asyncio porque..."

### Posibles Simulaciones del Profesor

**Simulaci√≥n 1: Desconexi√≥n de red**
```bash
# El profesor puede pedir:
docker-compose stop app
# ‚Üí Gateway detecta, cierra WebSocket, cliente ve error
```

**Simulaci√≥n 2: Sobrecarga del sistema**
```bash
# Muchos usuarios simult√°neos
# ‚Üí Rate limiter bloquea flooding
# ‚Üí Sem√°foro limita llamadas al LLM
# ‚Üí Sistema se mantiene estable
```

**Simulaci√≥n 3: Cliente lento**
```bash
# Un cliente tarda en leer
# ‚Üí NO bloquea a otros clientes
# ‚Üí writer.drain() implementa backpressure
```

---

## Checklist Final Pre-Defensa

### Conceptos Fundamentales
- [ ] He le√≠do todo el documento DEFENSA_PROYECTO.md
- [ ] Puedo explicar qu√© es un socket sin mirar apuntes
- [ ] Puedo explicar la diferencia entre concurrencia y paralelismo
- [ ] S√© d√≥nde est√° el socket en el c√≥digo (`app/server.py`)
- [ ] Entiendo por qu√© usamos asyncio (I/O-bound)
- [ ] Puedo explicar c√≥mo identificamos usuarios (Usuario-N)
- [ ] Entiendo el flujo completo (navegador ‚Üí gateway ‚Üí servidor ‚Üí LLM)
- [ ] S√© qu√© es el event loop conceptualmente
- [ ] Entiendo c√≥mo TCP garantiza orden (n√∫meros de secuencia)
- [ ] Puedo justificar Docker (reproducibilidad + aislamiento)

### Implementaci√≥n Espec√≠fica
- [ ] S√© c√≥mo funciona el rate limiter (ventana deslizante)
- [ ] Entiendo el sem√°foro (limita concurrencia al LLM)
- [ ] Puedo explicar qu√© hace `writer.drain()`
- [ ] Conozco la funci√≥n del trace_id (Usuario-N:mM)
- [ ] Entiendo el ajuste de ventana de tokens
- [ ] S√© c√≥mo funcionan los reintentos con backoff exponencial

### Operaciones y Limitaciones
- [ ] Puedo listar las 10 operaciones admitidas del sistema
- [ ] Puedo explicar las 7 limitaciones conocidas
- [ ] S√© justificar por qu√© tenemos esas limitaciones
- [ ] Puedo explicar c√≥mo resolver cada limitaci√≥n en producci√≥n
- [ ] Entiendo la diferencia entre prototipo educativo y sistema productivo

### Demos y Pruebas
- [ ] Puedo mostrar logs en vivo con `docker-compose logs -f`
- [ ] He probado el sistema con m√∫ltiples navegadores abiertos
- [ ] S√© ejecutar el script de carga (`load_test.py` conceptual)
- [ ] Puedo demostrar el rate limiting en acci√≥n
- [ ] S√© simular una desconexi√≥n de red con Docker

### Preguntas Cr√≠ticas del Profesor
- [ ] Puedo responder "¬øC√≥mo funciona el servidor?" (pregunta 1)
- [ ] Puedo responder "¬øC√≥mo distingu√≠s usuarios?" (identificaci√≥n)
- [ ] Puedo responder "¬øPor qu√© asyncio?" (justificaci√≥n t√©cnica)
- [ ] Puedo responder "¬øQu√© pasa si dos usuarios escriben juntos?" (aislamiento)
- [ ] Puedo responder "¬øCu√°les son las limitaciones?" (pregunta 20)
- [ ] Puedo responder sobre simulaci√≥n de red (Docker + tc)

---

**Autor**: M√°ximo Catal√°n  
**Fecha**: Noviembre 2025  
**Repositorio**: [PsicoIA](https://github.com/Catalan-Maximo/PsicoIA)

---

## üéØ √öltima Recomendaci√≥n

**PRACTICA explicando en voz alta** cada secci√≥n. Si pod√©s explicarlo sin mirar, lo domin√°s. Si dud√°s, rele√© esa parte hasta que sea natural.

**¬°Mucha suerte en la defensa! üöÄ**
