# PsicoIA â€” Chatbot de apoyo emocional (TCP + WebSocket)

Proyecto multiusuario basado en asyncio que expone un servidor TCP (`app`) y un gateway WebSocket (`gateway`).
Por cada cliente WebSocket el gateway abre una conexiÃ³n TCP dedicada hacia la `app`, garantizando sesiones aisladas.
Las solicitudes al LLM se realizan mediante un cliente async (httpx) hacia un endpoint compatible con OpenAI (por ejemplo, Groq).

## Contenido rÃ¡pido

- Servidor TCP asÃ­ncrono para mÃºltiples usuarios (PUERTO por defecto: `5001`).
- Gateway WebSocket que actÃºa como puente entre navegador y servidor TCP (PUERTO por defecto: `8765`).
- Contenedores Docker y `docker compose` para desplegar localmente.

## Requisitos

- Docker Desktop o Docker Engine
- Docker Compose (incluido con Docker Desktop)
- Clave de API compatible (por ejemplo, Groq API key)
- Navegador moderno para `web/client_web.html` (opcional)

## Variables de entorno

1. Copia `.env.example` â†’ `.env` y completa los valores.

Ejemplo clave (informativo):

```env
# App
APP_HOST=0.0.0.0
APP_PORT=5001
MAX_IN_FLIGHT=50
PER_USER_MAX=2
RATE_WINDOW_SECONDS=10
RATE_MAX_MESSAGES=6

# LLM
GROQ_API_KEY=gsk_...tu_clave...
MODEL_NAME=meta-llama/llama-3.1-8b-instant
# (Opcional) LLM_URL=https://mi-endpoint/openai/v1/chat/completions
```

---

## Ejecutar:
### OpciÃ³n A: Docker (recomendada)

Desde la raÃ­z del proyecto (con Docker activo):

```powershell
docker compose build
docker compose up -d
```

Ver servicios y puertos:

```powershell
docker compose ps
```

Ver logs:

```powershell
docker compose logs -f app
docker compose logs -f gateway
```

Detener todo:

```powershell
docker compose down
```

Reconstruir despuÃ©s de cambios en el cÃ³digo:

```powershell
docker compose up -d --build
```

### OpciÃ³n B: Consola/Terminal

Desde la raÃ­z del proyecto:

```powershell
python -m app.server
#y luego en otra consola/terminal
python -m gateway.ws_gateway
```

Para detener:

```powershell
#Ctrl + C, en ambas consolas
^C
```

## Probar la aplicaciÃ³n

### **OpciÃ³n 1: Cliente web HTML (interfaz grÃ¡fica - RECOMENDADO)**

1. AsegÃºrate de que `app` y `gateway` estÃ©n levantados (con Docker o localmente).
2. Abre el archivo `web/client_web.html` directamente en tu navegador (no requiere servidor web).
3. El cliente se conectarÃ¡ automÃ¡ticamente a `ws://localhost:8765`.
4. Escribe mensajes en el chat y recibe respuestas del chatbot.
5. **Cada pestaÃ±a del navegador = usuario independiente** con su propia sesiÃ³n.

---

### **OpciÃ³n 2: Consola TCP directa (lÃ­nea de comandos)**

Conecta directamente al servidor TCP sin pasar por el gateway WebSocket.

#### **Windows (PowerShell)**
















## ğŸ”„ Flujo de datos y arquitectura

### **Arquitectura general**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     WebSocket      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      TCP       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTP       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Navegador  â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ Gateway  â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   App   â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   LLM   â”‚
â”‚ (HTML+JS)   â”‚   ws://localhost:   â”‚  (WSâ†”TCP)â”‚   localhost:   â”‚ (Server)â”‚   API (Groq/   â”‚ (Groq/  â”‚
â”‚             â”‚        8765         â”‚          â”‚      5001      â”‚         â”‚    OpenAI)     â”‚ OpenAI) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                                   â”‚                            â”‚
     â”‚                                   â”‚                            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Cada usuario tiene sesiÃ³n aislada (1:1:1)
```

### **Flujo de un mensaje (paso a paso)**

1. **Usuario envÃ­a mensaje**:
   - Desde navegador (WebSocket) o consola TCP directa.

2. **Gateway WebSocket** (si se usa):
   - Recibe mensaje del navegador vÃ­a WebSocket.
   - Lo reenvÃ­a a travÃ©s de su conexiÃ³n TCP dedicada al servidor `app`.

3. **Servidor TCP (`app/server.py`)**:
   - Acepta la conexiÃ³n y crea una coroutine `handle_client()`.

4. **Client Handler (`app/client_handler.py`)**:
   - Asigna ID de usuario (`Usuario-N`).
   - Valida rate limit (ventana deslizante).
   - Genera `trace_id` para trazabilidad.
   - Espera en el semÃ¡foro global (`MAX_IN_FLIGHT`).

5. **LLM Client (`app/services/llm_client.py`)**:
   - Recupera historial de conversaciÃ³n del usuario desde RAM.
   - Construye array de `messages` con ventana de tokens.
   - EnvÃ­a POST HTTP asÃ­ncrono al LLM (Groq/OpenAI).
   - Aplica reintentos con backoff si hay errores 429/5xx.
   - Guarda respuesta en historial.

6. **Respuesta al usuario**:
   - `llm_client.py` devuelve texto al `client_handler.py`.
   - `client_handler.py` envÃ­a respuesta por TCP.
   - `gateway` (si se usa) reenvÃ­a por WebSocket al navegador.
   - El navegador o terminal muestra la respuesta.

### **Modelo de concurrencia**

- **asyncio**: Un solo proceso, un solo hilo, event loop no bloqueante.
- **Coroutines**: Cada conexiÃ³n TCP obtiene su propia coroutine `handle_client()`.
- **I/O asÃ­ncrono**:
  - `await reader.readline()`: Lee sin bloquear otras conexiones.
  - `await httpx.post()`: Llama al LLM sin bloquear el servidor.
  - `await writer.drain()`: Escribe sin bloquear.
- **ProtecciÃ³n**:
  - **SemÃ¡foro global**: Limita requests simultÃ¡neos al LLM.
  - **Rate limiter por usuario**: Previene flooding individual.

### **GestiÃ³n de estado**

- **Historial de conversaciÃ³n**: Almacenado en RAM en diccionario `_histories` (clave: `conversation_id`).
- **Locks por conversaciÃ³n**: `asyncio.Lock` evita race conditions al modificar historiales.
- **Ventana de tokens**: Solo se envÃ­an los mensajes mÃ¡s recientes que caben en `LLM_INPUT_TOKEN_BUDGET`.
- **Persistencia**: No hay; si reinicias el servidor, se pierden los historiales (puedes implementar DB).

---
